{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell0",
   "metadata": {},
   "source": [
    "Learning to rank.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d580da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wvpy.jtools import declare_task_variables\n",
    "\n",
    "# set up for external override\n",
    "with declare_task_variables(globals()):\n",
    "    rand_seed = 2024\n",
    "    do_display = True\n",
    "    result_fname = \"\"\n",
    "    m_examples = 100\n",
    "    score_name = \"quality\"\n",
    "    clean_up = False\n",
    "    show_console = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up Python\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy.stats import spearmanr\n",
    "from cmdstanpy import CmdStanModel\n",
    "from plotnine import *\n",
    "from rank_plotting_fns import (\n",
    "    define_Stan_choice_src,\n",
    "    define_Stan_list_src,\n",
    "    define_Stan_inspection_src,\n",
    "    estimate_model_from_scores,\n",
    "    format_Stan_data,\n",
    "    format_Stan_inspection_data,\n",
    "    mk_example,\n",
    "    plot_rank_performance,\n",
    "    run_stan_model,\n",
    "    sort_observations_frame,\n",
    "    XgboostClassifier,\n",
    ")\n",
    "\n",
    "# quiet down Stan\n",
    "logger = logging.getLogger(\"cmdstanpy\")\n",
    "logger.addHandler(logging.NullHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(rand_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell3",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_name = \"uci wine example\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_display:\n",
    "    print(example_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read data and set scale of system\n",
    "features_frame = pd.read_csv(\"uci_wine_example_features.csv\")\n",
    "features_scores = pd.read_csv(\"uci_wine_example_scores.csv\")\n",
    "m_train_examples: int = m_examples\n",
    "m_test_examples: int = m_examples\n",
    "noise_scale = 0.87\n",
    "continue_inspection_probability = 0.8\n",
    "\n",
    "know_score: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell6",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35eb1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_stats = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_alternatives: int = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e917c543",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vars = features_frame.shape[1] + n_alternatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "continue_inspection_probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell10",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations_train = mk_example(\n",
    "    features_frame=features_frame,\n",
    "    features_scores=features_scores,\n",
    "    continue_inspection_probability=continue_inspection_probability,\n",
    "    n_alternatives=n_alternatives,\n",
    "    m_examples=m_train_examples,\n",
    "    score_name=score_name,\n",
    "    noise_scale=noise_scale,\n",
    "    rng=rng,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d96bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations_test = mk_example(\n",
    "    features_frame=features_frame,\n",
    "    features_scores=features_scores,\n",
    "    continue_inspection_probability=continue_inspection_probability,\n",
    "    n_alternatives=n_alternatives,\n",
    "    m_examples=m_test_examples,\n",
    "    score_name=score_name,\n",
    "    noise_scale=noise_scale,\n",
    "    rng=rng,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1527a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "perfect_coef = None\n",
    "if know_score:\n",
    "    perfect_coef = estimate_model_from_scores(\n",
    "            observations=observations_train,\n",
    "            features_frame=features_frame,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faade9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure we don't have a column we would not know in practice\n",
    "observations_train = observations_train.loc[\n",
    "    :, [c for c in observations_train.columns if not c.startswith(\"score_value_\")]\n",
    "].reset_index(drop=True, inplace=False)\n",
    "observations_test = observations_test.loc[\n",
    "    :, [c for c in observations_test.columns if not c.startswith(\"score_value_\")]\n",
    "].reset_index(drop=True, inplace=False)\n",
    "\n",
    "observations_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell11",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_frequencies = (\n",
    "    observations_train\n",
    "        .loc[\n",
    "            :, \n",
    "            [c for c in observations_train.columns if c.startswith(\"pick_value_\")]]\n",
    "        .mean(axis=0)\n",
    "        .reset_index(drop=False, inplace=False)\n",
    "        .sort_values(['index'], ignore_index=True)\n",
    ")\n",
    "win_frequencies.columns = ['position', 'win frequency']\n",
    "\n",
    "win_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ed9931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: better estimate here\n",
    "p_continue_est = np.mean( \n",
    "    (np.array(win_frequencies['win frequency'][range(1, win_frequencies.shape[0])]) + 1e-2)\n",
    " / (np.array(win_frequencies['win frequency'][range(0, win_frequencies.shape[0] - 1)]) + 1e-2)\n",
    ")\n",
    "\n",
    "p_continue_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the observable data\n",
    "observations_train[\n",
    "    [c for c in observations_train.columns if not c.startswith(\"display_position_\")]\n",
    "].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell14",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_compare_frame = pd.DataFrame([[] for i in range(features_frame.shape[0])])\n",
    "if know_score:\n",
    "    score_compare_frame[\"hidden concept\"] = features_scores[\n",
    "        score_name\n",
    "    ]  # would not know this for non-synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell15",
   "metadata": {},
   "source": [
    "Try a Stan model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell22",
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_model_inspection_src = define_Stan_inspection_src(n_alternatives=n_alternatives)\n",
    "if do_display:\n",
    "    print(stan_model_inspection_src)\n",
    "data_inspection_str = format_Stan_inspection_data(\n",
    "    observations=observations_train,\n",
    "    features_frame=features_frame,\n",
    "    p_continue=p_continue_est,\n",
    ")\n",
    "fit_inspection = run_stan_model(\n",
    "    stan_model_src=stan_model_inspection_src,\n",
    "    data_str=data_inspection_str,\n",
    "    model_note='inspection_model',\n",
    "    clean_up=clean_up,\n",
    "    show_console=show_console,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get implied sample weights from chain\n",
    "wt_frame = fit_inspection.draws_pd(vars=[\"lp__\"])\n",
    "\n",
    "if do_display:\n",
    "    stddev = np.sqrt(np.var(wt_frame[\"lp__\"]))\n",
    "    log_samples = np.log(wt_frame.shape[0])\n",
    "    (\n",
    "        ggplot(\n",
    "            data=wt_frame,\n",
    "            mapping=aes(x=\"lp__\"),\n",
    "        )\n",
    "        + geom_density(fill=\"gray\", alpha=0.7)\n",
    "        + ggtitle(\n",
    "            f\"{example_name} Stan lp__ value on inspection draws\\nstandard deviation: {stddev:.2f}, log samples = {log_samples:.2f}\"\n",
    "        )\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell26",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_draws = fit_inspection.draws_pd(vars=[\"beta\"])\n",
    "beta_draws_display = beta_draws.copy()\n",
    "beta_draws_display.columns = list(features_frame.columns) \n",
    "\n",
    "beta_draws_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this this entirety of what we pull out of Stan- per modeled preference cohort\n",
    "# from now on we do not use Stan\n",
    "estimated_beta_inspection_Stan = beta_draws_display.loc[\n",
    "    wt_frame[\"lp__\"] >= np.quantile(wt_frame[\"lp__\"], 0.5), :\n",
    "].mean()\n",
    "estimated_beta_inspection_Stan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f409e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_beta_inspection_Stan = list(estimated_beta_inspection_Stan) + [0] * n_alternatives  # TODO: clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell29",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_pull_inspection_Stan = plot_rank_performance(\n",
    "    model=estimated_beta_inspection_Stan,  # estimated coefficients\n",
    "    model_type='coef',\n",
    "    example_name=example_name,  # name of data set\n",
    "    n_alternatives=n_alternatives,  # size of lists\n",
    "    features_frame=features_frame,  # features by row id\n",
    "    observations_train=observations_train,  # training observations layout frame\n",
    "    observations_test=observations_test,  # evaluation observations layout frame\n",
    "    estimate_name=\"Stan inspection model\",  # display name of estimate\n",
    "    score_compare_frame=score_compare_frame,  # score comparison frame (altered by call)\n",
    "    rng=rng,  # pseudo random source\n",
    "    show_plots=do_display,\n",
    ")\n",
    "collected_stats.append(stat_pull_inspection_Stan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731bc70f",
   "metadata": {},
   "source": [
    "Try a Stan model with position choice modeled as utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba91cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations_sorted_train = sort_observations_frame(observations_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23298cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_model_list_src = define_Stan_list_src(n_alternatives=n_alternatives)\n",
    "if do_display:\n",
    "    print(stan_model_list_src)\n",
    "data_str = format_Stan_data(\n",
    "    observations_sorted=observations_sorted_train,\n",
    "    features_frame=features_frame,\n",
    ")\n",
    "fit_utility_Stan = run_stan_model(\n",
    "    stan_model_src=stan_model_list_src,\n",
    "    data_str=data_str,\n",
    "    model_note='utility_model',\n",
    "    clean_up=clean_up,\n",
    "    show_console=show_console,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8413e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_draws_utility = fit_utility_Stan.draws_pd(vars=[\"beta\"])\n",
    "beta_draws_utility_display = beta_draws_utility.copy()\n",
    "beta_draws_utility_display.columns = list(features_frame.columns) + [\n",
    "    f'position_{sel_j}' for sel_j in range(n_alternatives)\n",
    "]\n",
    "\n",
    "beta_draws_utility_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bac55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get implied sample weights from chain\n",
    "wt_frame_utility = fit_utility_Stan.draws_pd(vars=[\"lp__\"])\n",
    "\n",
    "if do_display:\n",
    "    stddev = np.sqrt(np.var(wt_frame_utility[\"lp__\"]))\n",
    "    log_samples = np.log(wt_frame_utility.shape[0])\n",
    "    (\n",
    "        ggplot(\n",
    "            data=wt_frame_utility,\n",
    "            mapping=aes(x=\"lp__\"),\n",
    "        )\n",
    "        + geom_density(fill=\"gray\", alpha=0.7)\n",
    "        + ggtitle(\n",
    "            f\"{example_name} Stan lp__ value on utility draws\\nstandard deviation: {stddev:.2f}, log samples = {log_samples:.2f}\"\n",
    "        )\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ae53e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_beta_utility_Stan = beta_draws_utility_display.loc[\n",
    "    wt_frame[\"lp__\"] >= np.quantile(wt_frame_utility[\"lp__\"], 0.5), :\n",
    "].mean()\n",
    "estimated_beta_utility_Stan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467dd51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_pull_Stan = plot_rank_performance(\n",
    "    model=estimated_beta_utility_Stan,  # estimated coefficients\n",
    "    model_type='coef',\n",
    "    example_name=example_name,  # name of data set\n",
    "    n_alternatives=n_alternatives,  # size of lists\n",
    "    features_frame=features_frame,  # features by row id\n",
    "    observations_train=observations_train,  # training observations layout frame\n",
    "    observations_test=observations_test,  # evaluation observations layout frame\n",
    "    estimate_name=\"Stan utility model\",  # display name of estimate\n",
    "    score_compare_frame=score_compare_frame,  # score comparison frame (altered by call)\n",
    "    rng=rng,  # pseudo random source\n",
    "    show_plots=do_display,\n",
    ")\n",
    "collected_stats.append(stat_pull_Stan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b295a6c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cell30",
   "metadata": {},
   "source": [
    "Try to approximate the Stan model with a classification model with similar error structure.\n",
    "Consider each pair of list entries with a different outcome as an observation and try to\n",
    "build a model that reproduces the observed outcomes.\n",
    "The extra trick is: repeat the whole data frame negated with the outcomes reverse (so \n",
    "we don't define a problem with all positive or all negative outcomes).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_names = list(features_frame.columns) + [\n",
    "    f\"position_{sel_i}\" for sel_i in range(n_alternatives)\n",
    "]\n",
    "enc_frame = []\n",
    "for row_i in range(observations_train.shape[0]):\n",
    "    feature_row = observations_train.loc[row_i, :]\n",
    "    sel_pick = np.argmax(\n",
    "        feature_row[[f\"pick_value_{sel_i}\" for sel_i in range(n_alternatives)]]\n",
    "    )\n",
    "    for sel_i in range(n_alternatives):\n",
    "        if sel_i != sel_pick:\n",
    "            posn_vec = [0] * n_alternatives\n",
    "            posn_vec[sel_pick] = 1.0\n",
    "            posn_vec[sel_i] = -1.0\n",
    "            encoded_row = list(\n",
    "                    features_frame.loc[feature_row[f\"item_id_{sel_pick}\"], :]\n",
    "                    - features_frame.loc[feature_row[f\"item_id_{sel_i}\"], :]\n",
    "                ) + posn_vec  \n",
    "            di = pd.DataFrame({k: [v] for k, v in zip(feature_names, encoded_row)})\n",
    "            enc_frame.append(di)\n",
    "enc_frame = pd.concat(enc_frame, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell33",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = LogisticRegression(max_iter=10000, solver=\"newton-cholesky\")\n",
    "classification_model_name = 'logistic'\n",
    "\n",
    "# classification_model = XgboostClassifier(rng=rng)\n",
    "# classification_model_name = 'xgboost'\n",
    "\n",
    "# classification_model = RandomForestClassifier()\n",
    "# classification_model_name = 'Random Forest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cacaa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classification_model.fit(\n",
    "    X=pd.concat([enc_frame, -enc_frame], ignore_index=True),\n",
    "    y=[True] * enc_frame.shape[0] + [False] * enc_frame.shape[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c98b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell34",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_pull_classifier = plot_rank_performance(\n",
    "    model=classification_model,\n",
    "    model_type='classifier',\n",
    "    example_name=example_name,  # name of data set\n",
    "    n_alternatives=n_alternatives,  # size of lists\n",
    "    features_frame=features_frame,  # features by row id\n",
    "    observations_train=observations_train,  # training observations layout frame\n",
    "    observations_test=observations_test,  # evaluation observations layout frame\n",
    "    estimate_name=classification_model_name,  # display name of estimate\n",
    "    score_compare_frame=score_compare_frame,  # score comparison frame (altered by call)\n",
    "    rng=rng,  # pseudo random source\n",
    "    show_plots=do_display,\n",
    ")\n",
    "collected_stats.append(stat_pull_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecacdbf",
   "metadata": {},
   "source": [
    "We can compare this to just looking at the scores (not implementable in real world problems)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e957f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plug in the perfect model that saw the scores\n",
    "if know_score and (perfect_coef is not None):\n",
    "    perfect_pull = plot_rank_performance(\n",
    "        model=perfect_coef,  # estimated coefficients\n",
    "        model_type='coef',  # TODO pass regression model directly\n",
    "        example_name=example_name,  # name of data set\n",
    "        n_alternatives=n_alternatives,  # size of lists\n",
    "        features_frame=features_frame,  # features by row id\n",
    "        observations_train=observations_train,  # training observations layout frame\n",
    "        observations_test=observations_test,  # evaluation observations layout frame\n",
    "        estimate_name=\"observed score\",  # display name of estimate\n",
    "        score_compare_frame=score_compare_frame,  # score comparison frame (altered by call)\n",
    "        rng=rng,  # pseudo random source\n",
    "        show_plots=do_display,\n",
    "    )\n",
    "    collected_stats.append(perfect_pull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1f4e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_stats = pd.concat(collected_stats, ignore_index=True)\n",
    "# collected_stats['perfect_model_SpearmanR'] = perfect_model_SpearmanR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc86c70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ff9bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (result_fname is not None) and (len(result_fname) > 0):\n",
    "    collected_stats.to_csv(result_fname, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bd1049",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_display:\n",
    "    display(collected_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6371ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prob_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
