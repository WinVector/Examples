{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This note works an example of partial pooling from equation 12.1 of Gelman, Hill, \"Data Analysis Using Regression and Multilevel/Hierarchical Models\", Cambridge 2007.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The topic of formula 12.1 is: how to partially pool data by hand, though this task is often handled by a hierarchical model solver. \n",
    "\n",
    "The idea is: we want to estimate the ideal mean of a value from a few observations at a location. The optimal linear un-biased estimator is to just take the average of the observed values, and hope this is close to the ideal unobserved mean.\n",
    "\n",
    "The wrinkle in partial pooling is: if we have data from other (different buy related) locations, can we use that to improve our estimate?\n",
    "\n",
    "Let's work this example with a few simplifying assumptions, using the `Python` `sympy` package to do the algebra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import sympy\n",
    "from sympy.stats import E, Normal\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our idea is:\n",
    "\n",
    "  * Each location has an unobserved mean value of examples drawn from this location, call this `LocationValue_j`.\n",
    "  * The locations are related, in that the `LocationValue_j`s are all drawn from some common distribution. This is why we think pooling data could be useful.\n",
    "  * For a given location, we figure some combination of the observations from the location, plus observations from other locations may be a lower expected error estimate than can be found using only observations from the location.\n",
    "\n",
    "To execute this idea we need to define a great number of variables and their relations as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define initial variables\n",
    "MeanLocationValue = sympy.Symbol(\"MeanLocationValue\")  # center of distribution generating garages, unobserved\n",
    "n_obs = sympy.Symbol(\"n_j\", positive=True)  # total number of observations across all garages, observed.\n",
    "BetweenLocationsSD = sympy.Symbol(\"BetweenLocationsSD\")  # how garages very from each other in expected behavior, unobserved\n",
    "ObservedMean = Normal(\"ObservedMean\", mean=MeanLocationValue, std = BetweenLocationsSD / sympy.sqrt(n_obs))  # mean of all observations, observed\n",
    "LocationValue_j = sympy.Symbol(\"LocationValue_j\")  # actual expected behavior of a given garage, unobserved and the goal to estimate\n",
    "LocationDistFactor_j = Normal(\"LocationDistFactor_j\", mean=0, std=BetweenLocationsSD)  # how locations differ from each other, unobserved\n",
    "def_LocationValue_j = MeanLocationValue + LocationDistFactor_j  #  generative definition of LocationDistFactor_j\n",
    "PerObservationSD = sympy.Symbol(\"PerObservationSD\", positive=True)  # sd of distribution generating observations, unobserved\n",
    "n_j = sympy.Symbol(\"n_j\", positive=True)  # number of observations at the j-th garage, observed\n",
    "LocationMean_j = sympy.Symbol(\"LocationMean_j\", mean=LocationValue_j, std=PerObservationSD)  # mean of all observations at garage j, observed\n",
    "LocationCenterNoise_ji = Normal(\"LocationCenterNoise_ji\", mean=0, std = PerObservationSD / sympy.sqrt(n_j))  # how mean observations at given location vary, unobserved\n",
    "def_LocationMean_j = LocationValue_j + LocationCenterNoise_ji  # generative definition of LocationCenterNoise_ji\n",
    "w = sympy.Symbol(\"w\", positive=True)  # our weighting term picking how to pool specific and general observations, to solve for\n",
    "estimate_j = sympy.Symbol(\"estimate_j\")  # our estimate of the behavior of the j-th garage, to solve for\n",
    "def_estimate_j = w * LocationMean_j + (1-w) * ObservedMean  # definition of our estimate\n",
    "expected_error_term = LocationValue_j - estimate_j  # error of our estimate, to minimize square of"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula 12.1 from Gelman and Hill is as follows.\n",
    "\n",
    "<img src=\"IMG_1323.png\">\n",
    "\n",
    "This is a big equation with a \"pooling strength\" coefficient expanded into the expression. If we write the pooling coefficient as `w`, we can then re-write the above as the follosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle LocationMean_{j} w + \\left(1 - w\\right) ObservedMean$"
      ],
      "text/plain": [
       "LocationMean_j*w + (1 - w)*ObservedMean"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def_estimate_j"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is: our estimate is `w` times using the observed per-location mean (`LocationMean_j`, the obvious estimate) plus `(1-w)` times the observed mean of all observations from all locations. Setting `w = 1` gives us the traditional \"use only observations from the chosen location\" solution. Setting `w = 0` gives us complete pooling, assuming there is not difference between locations. The trick is to find a `w` between `0` and `1` that might have lower expected square error. \n",
    "\n",
    "The 12.1 solution is a solution that picks `w` as the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{1}{1 + \\frac{PerObservationSD^{2}}{BetweenLocationsSD^{2} n_{j}}}$"
      ],
      "text/plain": [
       "1/(1 + PerObservationSD**2/(BetweenLocationsSD**2*n_j))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neat_soln_approx = 1 / (1 + PerObservationSD**2 / (n_j * BetweenLocationsSD**2))\n",
    "\n",
    "neat_soln_approx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This solution for `w` has some nice properties.\n",
    "\n",
    "  * `w` goes to `1` (the standard simple solution) as `PerObservationSD` goes to zero. This can be read as: \"there is no point in pooling of their is already little uncertainty in the obvious estimate.\n",
    "  * `w` goes to `1` (the standard simple solution) as `n_j` goes to infinity. This can be read as: \"there is no point in pooling if we already have a lot of data for the obvious estimate.\n",
    "  * `w` goes to `0` (combining all the data) as `PerObservationSD` goes to infinity. This can be read as: \"combine all the data if the per-location uncertainty is very high.\"\n",
    "\n",
    "The goal is to then derive this solution. First we will derive a similar solution, and then the identical solution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The exact solution\n",
    "\n",
    "We can solve for `w` by performing some substitutions in to our error term. The goal is to minimize the square of this error term, which counts negative errors as also being bad.\n",
    "\n",
    "The error term can be written as `A - B - C` where `A`, `B`, and `C` are as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = (1-w) * LocationDistFactor_j\n",
    "B = w * LocationCenterNoise_ji\n",
    "C = (1 - w) * (ObservedMean - MeanLocationValue)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm this as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_term_exact = (\n",
    "    expected_error_term\n",
    "        .subs(estimate_j, def_estimate_j)  # definition of estimate_j\n",
    "        .subs(LocationMean_j, def_LocationMean_j)  # generative definition of LocationMean_j\n",
    "        .subs(LocationValue_j, def_LocationValue_j)  # generative definition of LocationValue_j\n",
    ").expand().simplify()\n",
    "assert (error_term_exact - (A - B - C)).expand() == 0\n",
    "assert (E(C*C) - (1 - w)**2 * BetweenLocationsSD**2 / n_j).expand() == 0\n",
    "assert E(A).expand() == 0\n",
    "assert E(B).expand() == 0\n",
    "assert E(C).expand() == 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are assuming each of the random variables `A`, `B`, and `C` are independent of the others and expected value `0`. This means `E[(A - B - C)**2] = E[A**2] + E[B**2] + E[C**2]`. We can then solve for where the derivative of this is zero to get the optimal value for `w`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{BetweenLocationsSD^{2} \\left(n_{j} + 1\\right)}{BetweenLocationsSD^{2} n_{j} + BetweenLocationsSD^{2} + PerObservationSD^{2}}$"
      ],
      "text/plain": [
       "BetweenLocationsSD**2*(n_j + 1)/(BetweenLocationsSD**2*n_j + BetweenLocationsSD**2 + PerObservationSD**2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soln_exact = sympy.solve(sympy.diff((E(A * A) + E(B * B) + E(C * C)).expand(), w), w)[0]\n",
    "\n",
    "soln_exact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can neaten this solution for `w` up a bit as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{1}{1 + \\frac{PerObservationSD^{2}}{BetweenLocationsSD^{2} \\left(n_{j} + 1\\right)}}$"
      ],
      "text/plain": [
       "1/(1 + PerObservationSD**2/(BetweenLocationsSD**2*(n_j + 1)))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neat_soln_exact = 1 / (1 + PerObservationSD**2 / ((n_j + 1) * BetweenLocationsSD**2))\n",
    "assert (soln_exact - neat_soln_exact).together().expand() == 0\n",
    "\n",
    "neat_soln_exact"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some algebra shows that this differs from the Gelman and Hill solution only in that we have an `(n_j + 1)` where they have an `n_j`.\n",
    "\n",
    "## Reproducing the Gelman and Hill solution\n",
    "\n",
    "We can match the Gelman and Hill solution by, during the solving, replacing the visible `MeanLocationValue` with our estimated `ObservedMean` (ignoring the small difference between them).\n",
    "\n",
    "When we solve in that matter we get the Gelman and Hill `w` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_term_approx = (\n",
    "    expected_error_term\n",
    "        .subs(estimate_j, def_estimate_j)  # definition of estimate_j\n",
    "        .subs(ObservedMean, MeanLocationValue)  # this step is an approximation, using the unobserved MeanLocationValue as if it is the observed ObservedMean\n",
    "        .subs(LocationMean_j, def_LocationMean_j)  # generative definition of LocationMean_j\n",
    "        .subs(LocationValue_j, def_LocationValue_j)   # generative definition of LocationValue_j\n",
    ").expand().simplify()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We claim this error term is equal to `A - B`, for the following `A`, `B`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = (1-w) * LocationDistFactor_j\n",
    "B = w * LocationCenterNoise_ji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (error_term_approx - (A - B)).simplify() == 0\n",
    "assert E(A).expand() == 0\n",
    "assert E(B).expand() == 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can expand `E[(A - B)**2]` as `E[A**2] + E[B**2]` (using the independence and mean-zero properties)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{BetweenLocationsSD^{2} n_{j}}{BetweenLocationsSD^{2} n_{j} + PerObservationSD^{2}}$"
      ],
      "text/plain": [
       "BetweenLocationsSD**2*n_j/(BetweenLocationsSD**2*n_j + PerObservationSD**2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soln_approx = sympy.solve(sympy.diff(E(A**2) + E(B**2), w), w)[0]\n",
    "\n",
    "soln_approx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{1}{1 + \\frac{PerObservationSD^{2}}{BetweenLocationsSD^{2} n_{j}}}$"
      ],
      "text/plain": [
       "1/(1 + PerObservationSD**2/(BetweenLocationsSD**2*n_j))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert (soln_approx - neat_soln_approx).together().expand() == 0\n",
    "\n",
    "neat_soln_approx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this, as promised matches the text book. I would suggest a slight preference for the exact solution over this one, thought the differences are small."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working an Example\n",
    "\n",
    "Let's see the partial pooling inference in action. \n",
    "\n",
    "We will generate the data according to the above hierarchical design. We will then see how a pooled estimate, itself using estimate parameters, can in fact improve results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(2023)  # set state of pseudo random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters to specific values\n",
    "example_between_locations_sd = 3.0\n",
    "example_per_observations_sd = 10.0\n",
    "n_locations = 10\n",
    "n_obs_per_location = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the unobserved location centers\n",
    "example_location_value_mean = rng.normal(loc=0, scale=15, size=1)\n",
    "example_locations = pd.DataFrame({\n",
    "    \"location_id\": range(n_locations),\n",
    "    \"effect\": rng.normal(loc=example_location_value_mean, scale=example_between_locations_sd, size=n_locations)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>effect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12.480676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4.947432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>9.691985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6.698217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>11.451937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>8.429935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>4.289738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>7.137952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>7.862549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9.176378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   location_id     effect\n",
       "0            0  12.480676\n",
       "1            1   4.947432\n",
       "2            2   9.691985\n",
       "3            3   6.698217\n",
       "4            4  11.451937\n",
       "5            5   8.429935\n",
       "6            6   4.289738\n",
       "7            7   7.137952\n",
       "8            8   7.862549\n",
       "9            9   9.176378"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the data\n",
    "observations = []\n",
    "for j in range(example_locations.shape[0]):\n",
    "    obs_j = pd.DataFrame({\n",
    "        \"location_id\": j,\n",
    "        \"observation_id\": range(n_obs_per_location),\n",
    "        \"observations\": rng.normal(loc=example_locations.effect[j], scale=example_per_observations_sd, size=n_obs_per_location),\n",
    "    })\n",
    "    observations.append(obs_j)\n",
    "observations = pd.concat(observations, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>observation_id</th>\n",
       "      <th>observations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.410240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13.802057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18.996457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.988453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.682149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.273959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>19.333211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.195023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>18.532085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>11.101462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.051770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.104755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>14.763563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>-9.948274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>20.678362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3.187026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.429608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>12.303175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>10.598554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>22.863785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    location_id  observation_id  observations\n",
       "0             0               0      3.410240\n",
       "1             0               1     13.802057\n",
       "2             1               0     18.996457\n",
       "3             1               1      8.988453\n",
       "4             2               0     -0.682149\n",
       "5             2               1      2.273959\n",
       "6             3               0     19.333211\n",
       "7             3               1     -0.195023\n",
       "8             4               0     18.532085\n",
       "9             4               1     11.101462\n",
       "10            5               0      0.051770\n",
       "11            5               1      1.104755\n",
       "12            6               0     14.763563\n",
       "13            6               1     -9.948274\n",
       "14            7               0     20.678362\n",
       "15            7               1      3.187026\n",
       "16            8               0     -1.429608\n",
       "17            8               1     12.303175\n",
       "18            9               0     10.598554\n",
       "19            9               1     22.863785"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standard estimator\n",
    "def standard_effect_estimate_1_held_out(*, location_id: int, observations: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Show the mean-square error of standard 1-held out estimates of the effect at location location_id from observations\n",
    "\n",
    "    :parm location_id: which location to calculate for\n",
    "    :param observations: the observations data frame\n",
    "    :return: 1-held out standard estimates\n",
    "    \"\"\"\n",
    "    # get observations in target group\n",
    "    obs = observations.loc[observations[\"location_id\"] == location_id, :].reset_index(drop=True, inplace=False)\n",
    "    # get the standard 1-held out estimates of the effect\n",
    "    observed_means = np.array([np.mean(obs.loc[obs.index != i, \"observations\"]) for i in range(obs.shape[0])])\n",
    "    return observed_means   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate the quality of the standard estimator\n",
    "def get_1_held_out_sq_error_of_direct_estimate_loss(location_id: int) -> float:\n",
    "    \"\"\"\n",
    "    Show the mean-square error of standard 1-held out estimates of the effect at location location_id from observations\n",
    "\n",
    "    :parm location_id: which location to calculate for\n",
    "    :return: estimated mean square error of this estimation procedure\n",
    "    \"\"\"\n",
    "    # get the estimates\n",
    "    observed_means = standard_effect_estimate_1_held_out(location_id=location_id, observations=observations)\n",
    "    # get the unobservable true effect for comparison\n",
    "    true_effect = example_locations.loc[example_locations[\"location_id\"] == location_id, \"effect\"].values[0]\n",
    "    # calculate the mean square error of these estimates\n",
    "    mean_square_error = np.mean((observed_means - true_effect)**2)\n",
    "    return mean_square_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the pooled estimator\n",
    "# a lot of the code complexity is the 1-hold out procedures\n",
    "# which are not needed to build the estimate, but to evaluate the quality of the estimate.\n",
    "def pooled_effect_estimate_1_held_out(*, location_id: int, observations: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Show the mean-square error of pooled 1-held out estimates of the effect at location location_id from observations\n",
    "\n",
    "    :parm location_id: which location to calculate for\n",
    "    :param observations: the observations data frame\n",
    "    :return: 1-held out pooled estimates\n",
    "    \"\"\"\n",
    "    # get observations in target group\n",
    "    obs = observations.loc[observations[\"location_id\"] == location_id, :].reset_index(drop=True, inplace=False)\n",
    "    # get the standard 1-held out estimates of the effect\n",
    "    observed_means = np.array([np.mean(obs.loc[obs.index != i, \"observations\"]) for i in range(obs.shape[0])])\n",
    "    # get the observed variance between locations\n",
    "    location_centers = observations.loc[:, [\"location_id\", \"observations\"]].groupby([\"location_id\"]).mean().reset_index(drop=False, inplace=False)\n",
    "    location_centers.rename(columns={\"observations\": \"centers\"}, inplace=True)\n",
    "    between_location_var = np.var(location_centers[\"centers\"], ddof=1)\n",
    "    # get the observed variance for each locations, excluding own location to make the out-of same\n",
    "    combined = observations.merge(\n",
    "        location_centers,\n",
    "        on=[\"location_id\"],\n",
    "        how=\"left\",\n",
    "        )\n",
    "    combined = combined.loc[combined[\"location_id\"] != location_id, :].reset_index(drop=True, inplace=False)\n",
    "    observation_var = np.sum((combined[\"observations\"] - combined[\"centers\"])**2) / (combined.shape[0] - len(set(combined[\"location_id\"])))\n",
    "    # get the mean of all observations, except self to strengthen hold-out strictness.\n",
    "    grand_mean = np.mean(combined[\"observations\"])\n",
    "    # get v, the pooling coefficient\n",
    "    n_j = obs.shape[0] - 1  # minus 1 to account for hold-out\n",
    "    v = 1 / (1 + observation_var / ((n_j + 1) * between_location_var))\n",
    "    # build the held-out pooled estimate\n",
    "    pooled_estimate = v * observed_means + (1 - v) * grand_mean\n",
    "    return pooled_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the pooled estimator quality\n",
    "def get_1_held_out_pooled_estimate_loss(location_id: int) -> float:\n",
    "    \"\"\"\n",
    "    Show the mean-square error of partial pooled 1-held out estimates of the effect at location location_id from observations\n",
    "\n",
    "    :parm location_id: which location to calculate for\n",
    "    :return: estimated mean square error of this estimation procedure\n",
    "    \"\"\"\n",
    "    # get the estimates\n",
    "    pooled_estimate = pooled_effect_estimate_1_held_out(location_id=location_id, observations=observations)\n",
    "    # get the unobservable true effect for comparison\n",
    "    true_effect = example_locations.loc[example_locations[\"location_id\"] == location_id, \"effect\"].values[0]\n",
    "    # calculate the mean square error of these estimates\n",
    "    mean_square_error = np.mean((pooled_estimate - true_effect)**2)\n",
    "    return mean_square_error\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try the estimators. In each case we the evaluation is returning mean-square-error, so smaller is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.00943220599963"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the experiment for the standard estimator\n",
    "std_est_loss = get_1_held_out_sq_error_of_direct_estimate_loss(0)\n",
    "\n",
    "std_est_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.265085844392072"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the experiment for the pooled estimator\n",
    "pooled_est_loss = get_1_held_out_pooled_estimate_loss(0)\n",
    "\n",
    "pooled_est_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert pooled_est_loss < std_est_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see, the pooled estimate has smaller expected 1-hold out mean square error loss that the standard estimate. That is: we have evidence the pooled estimate would be closer to new observations for a given location.\n",
    "\n",
    "The method has been seen to work.\n",
    "\n",
    "We used hold-out procedures on this data so we could estimate the future out of sample performance of the estimate. That is: to get a reliable estimate of if new observations would be near the inferred values or not. In production we would then run the procedure again with no hold-out to get a more statistically efficient estimate that is using more of the available data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The partial pooling improvement for estimating an unseen value from noisy observations depends on a single parameter `PerObservationSD**2 / ((n_j + 1) * BetweenLocationsSD**2)`. This parameter compares the uncertainty in the observations from a single location, to the uncertainty per-location, scaled by how many observations we have at the location in question. When this ratio is small, we don't pool data- we just estimate the average value using data from one location. When this ratio is large, pooling is likely a useful variance reducing procedure.\n",
    "\n",
    "Essentially the method is trading away variance (due to having very few samples) for bias (due to including samples that don't match the target location). If the different locations are sufficiently related, this can be an improving trade off in terms of expected square error.\n",
    "\n",
    "In practice the above inference is made inside a hierarchical model solver. However, it is good to see the expect form of the pooling strategy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1564fe6c621c255e1bf894f3aae80247d56f4153453d59260e9f5ebda34f6083"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
