{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This note works an example of partial pooling from equation 12.1 of Gelman, Hill, \"Data Analysis Using Regression and Multilevel/Hierarchical Models\", Cambridge 2007.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The topic of formula 12.1 is: how to partially pool data by hand, though this task is often handled by a hierarchical model solver. \n",
    "\n",
    "The idea is: we want to estimate the ideal mean of a value from a few observations at a location. The optimal linear un-biased estimator is to just take the average of the observed values, and hope this is close to the ideal unobserved mean.\n",
    "\n",
    "The wrinkle in partial pooling is: if we have data from other locations, can we use that to improve our estimate?\n",
    "\n",
    "Let's work this example with a few simplifying assumptions, using the Python sympy package to do the algebra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import sympy\n",
    "from sympy.stats import E, Normal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our idea is:\n",
    "\n",
    "  * Each location has an unobserved mean value of examples drawn from this location, call this `LocationValue_j`.\n",
    "  * The locations are related, in that the `LocationValue_j`s are all drawn from some common distribution, this is why we think pooling data could be useful.\n",
    "  * For a given location we figure some combination of the observations from the location, plus observations from other locations may be a lower expected error estimate than can be found using only observations from the location.\n",
    "\n",
    "To execute this idea we need to define a great number of variables and their relations as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define initial variables\n",
    "MeanLocationValue = sympy.Symbol(\"MeanLocationValue\")  # center of distribution generating garages, unobserved\n",
    "n_obs = sympy.Symbol(\"n_j\", positive=True)  # total number of observations across all garages, observed.\n",
    "BetweenLocationsSD = sympy.Symbol(\"BetweenLocationsSD\")  # how garages very from each other in expected behavior, unobserved\n",
    "ObservedMean = Normal(\"ObservedMean\", mean=MeanLocationValue, std = BetweenLocationsSD / sympy.sqrt(n_obs))  # mean of all observations, observed\n",
    "LocationValue_j = sympy.Symbol(\"LocationValue_j\")  # actual expected behavior of a given garage, unobserved and the goal to estimate\n",
    "LocationDistFactor_j = Normal(\"LocationDistFactor_j\", mean=0, std=BetweenLocationsSD)  # how locations differ from each other, unobserved\n",
    "def_LocationValue_j = MeanLocationValue + LocationDistFactor_j  #  generative definition of LocationDistFactor_j\n",
    "PerObservationSD = sympy.Symbol(\"PerObservationSD\", positive=True)  # sd of distribution generating observations, unobserved\n",
    "n_j = sympy.Symbol(\"n_j\", positive=True)  # number of observations at the j-th garage, observed\n",
    "LocationMean_j = sympy.Symbol(\"LocationMean_j\", mean=LocationValue_j, std=PerObservationSD)  # mean of all observations at garage j, observed\n",
    "LocationCenterNoise_ji = Normal(\"LocationCenterNoise_ji\", mean=0, std = PerObservationSD / sympy.sqrt(n_j))  # how mean observations at given location vary, unobserved\n",
    "def_LocationMean_j = LocationValue_j + LocationCenterNoise_ji  # generative definition of LocationCenterNoise_ji\n",
    "w = sympy.Symbol(\"w\", positive=True)  # our weighting term picking how to pool specific and general observations, to solve for\n",
    "estimate_j = sympy.Symbol(\"estimate_j\")  # our estimate of the behavior of the j-th garage, to solve for\n",
    "def_estimate_j = w * LocationMean_j + (1-w) * ObservedMean  # definition of our estimate\n",
    "expected_error_term = LocationValue_j - estimate_j  # error of our estimate, to minimize square of"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula 12.1 from Gelman and Hill is as follows.\n",
    "\n",
    "<img src=\"IMG_1323.png\">\n",
    "\n",
    "This actual a detailed form `w` from the following expression for our estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle LocationMean_{j} w + \\left(1 - w\\right) ObservedMean$"
      ],
      "text/plain": [
       "LocationMean_j*w + (1 - w)*ObservedMean"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def_estimate_j"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is our estimate is `w` times using the observed per-location mean (`LocationMean_j`, the obvious estimate) plus `1-w` times the observed mean of all observations from all locations. Setting `w = 1` gives us the traditional \"use only observations from the chosen location\" solution. The trick is to find a `w` between `0` and `1` that might have lower expected square error. \n",
    "\n",
    "The 12.1 solution is a solution that picks `w` as the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{1}{1 + \\frac{PerObservationSD^{2}}{BetweenLocationsSD^{2} n_{j}}}$"
      ],
      "text/plain": [
       "1/(1 + PerObservationSD**2/(BetweenLocationsSD**2*n_j))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neat_soln_approx = 1 / (1 + PerObservationSD**2 / (n_j * BetweenLocationsSD**2))\n",
    "\n",
    "neat_soln_approx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This solution for `w` has some nice properties.\n",
    "\n",
    "  * `w` goes to `1` (the standard simple solution) as `PerObservationSD` goes to zero. This can be read as: \"there is no point in pooling of their is already little uncertainty in the obvious estimate.\n",
    "  * `w` goes to `1` (the standard simple solution) as `n_j` goes to infinity. This can be read as: \"there is no point in pooling if we already have a lot of data for the obvious estimate.\n",
    "  * `w` goes to `0` (combining all the data) as `PerObservationSD` goes to infinity. This can be read as: \"combine all the data if the per-location uncertainty is very high.\"\n",
    "\n",
    "The goal is to then derive this solution. First we will derive a similar, solution and then the identical solution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The exact solution\n",
    "\n",
    "We can solve for `w` by performing some substitutions in to our error term. The goal is to minimize the square of this error term, which counts negative errors as also being bad.\n",
    "\n",
    "The error term can be written as `A - B - C` where `A`, `B`, and `C` are as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = (1-w) * LocationDistFactor_j\n",
    "B = w * LocationCenterNoise_ji\n",
    "C = (1 - w) * (ObservedMean - MeanLocationValue)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm this as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_term_exact = (\n",
    "    expected_error_term\n",
    "        .subs(estimate_j, def_estimate_j)  # definition of estimate_j\n",
    "        .subs(LocationMean_j, def_LocationMean_j)\n",
    "        .subs(LocationValue_j, def_LocationValue_j)\n",
    ").expand().simplify()\n",
    "assert (error_term_exact - (A - B - C)).expand() == 0\n",
    "assert (E(C*C) - (1 - w)**2 * BetweenLocationsSD**2 / n_j).expand() == 0\n",
    "assert E(A).expand() == 0\n",
    "assert E(B).expand() == 0\n",
    "assert E(C).expand() == 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are assuming each of the random variables `A`, `B`, and `C` are independent of the others and expected value `0`. This means `E[(A - B - C)**2] = E[A**2] + E[B**2] + E[C**2]`. We can then solve for where the derivative of this is zero to get the optimal value for `w`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{BetweenLocationsSD^{2} \\left(n_{j} + 1\\right)}{BetweenLocationsSD^{2} n_{j} + BetweenLocationsSD^{2} + PerObservationSD^{2}}$"
      ],
      "text/plain": [
       "BetweenLocationsSD**2*(n_j + 1)/(BetweenLocationsSD**2*n_j + BetweenLocationsSD**2 + PerObservationSD**2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soln_exact = sympy.solve(sympy.diff((E(A * A) + E(B * B) + E(C * C)).expand(), w), w)[0]\n",
    "\n",
    "soln_exact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can neaten this solution for `w` up a bit as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{1}{1 + \\frac{PerObservationSD^{2}}{BetweenLocationsSD^{2} \\left(n_{j} + 1\\right)}}$"
      ],
      "text/plain": [
       "1/(1 + PerObservationSD**2/(BetweenLocationsSD**2*(n_j + 1)))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neat_soln_exact = 1 / (1 + PerObservationSD**2 / ((n_j + 1) * BetweenLocationsSD**2))\n",
    "assert (soln_exact - neat_soln_exact).together().expand() == 0\n",
    "\n",
    "neat_soln_exact"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some algebra shows that this differs from the Gelman and Hill solution only in that we have an `(n_j + 1)` where they have an `n_j`.\n",
    "\n",
    "## Reproducing the Gelman and Hill solution\n",
    "\n",
    "We can match the Gelman and Hill solution by, during the solving, replacing the visible `MeanLocationValue` with our estimated `ObservedMean` (ignoring the small difference between them).\n",
    "\n",
    "When we solve in that matter we get the Gelman and Hill `w` as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_term_approx = (\n",
    "    expected_error_term\n",
    "        .subs(estimate_j, def_estimate_j)  # definition of estimate_j\n",
    "        .subs(ObservedMean, MeanLocationValue)  # this step is an approximation, using the unobserved MeanLocationValue as if it is the observed ObservedMean\n",
    "        .subs(LocationMean_j, def_LocationMean_j)\n",
    "        .subs(LocationValue_j, def_LocationValue_j)\n",
    ").expand().simplify()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = (1-w) * LocationDistFactor_j\n",
    "B = w * LocationCenterNoise_ji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (error_term_approx - (A - B)).simplify() == 0\n",
    "assert E(A).expand() == 0\n",
    "assert E(B).expand() == 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can expand `E[(A - B)**2]` as `E[A**2] + E[B**2]` (using the independence and mean-zero properties)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{BetweenLocationsSD^{2} n_{j}}{BetweenLocationsSD^{2} n_{j} + PerObservationSD^{2}}$"
      ],
      "text/plain": [
       "BetweenLocationsSD**2*n_j/(BetweenLocationsSD**2*n_j + PerObservationSD**2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soln_approx = sympy.solve(sympy.diff(E(A**2) + E(B**2), w), w)[0]\n",
    "\n",
    "soln_approx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{1}{1 + \\frac{PerObservationSD^{2}}{BetweenLocationsSD^{2} n_{j}}}$"
      ],
      "text/plain": [
       "1/(1 + PerObservationSD**2/(BetweenLocationsSD**2*n_j))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert (soln_approx - neat_soln_approx).together().expand() == 0\n",
    "\n",
    "neat_soln_approx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this, as promised matches the text book. I would suggest a slight preference for the exact solution over this one, thought the differences are small."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The partial pooling improvement for estimating an unseen value from noisy observations depends on a single parameter `PerObservationSD**2 / ((n_j + 1) * BetweenLocationsSD**2)`. This parameter compares the uncertainty in the observations from a single location, to the uncertainty per-location, scaled by how many observations we have at the location in question. When this ratio is small, we don't pool data- we just estimate the average value using data from one location. When this ratio is large, pooling is likely a useful variance reducing procedure.\n",
    "\n",
    "In practice the above inference is made inside a hierarchical model solver. However, it is good to see the expect form of the pooling strategy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1564fe6c621c255e1bf894f3aae80247d56f4153453d59260e9f5ebda34f6083"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
