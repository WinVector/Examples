---
title: "Followup: GAM-adjusting Multivariate Linear Models"
author: "Nina Zumel"
date: "2024-08-13"
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ggplot2) # for plotting
library(poorman) # for data wrangling
library(mgcv)    # for GAM
# library(lattice)

set.seed(20240812)
```

This is a followup on the article [*Post-hoc Adjustment for Zero-Thresholded Linear Models*](https://github.com/WinVector/Examples/blob/main/linear_regression_w_zeros/lm_adjust.md), in which I
showed how to use a one-variable GAM (e.g., a spline) to adjust a linear model in a problem space 
where outcomes are strictly nonnegative. If you haven't read that article, I suggest you check it out, first.

> When you donâ€™t expect to see too many zeros in practice, modeling the process as linear and thresholding negative predictions at zero is not unreasonable. But the more zeros (saturations) you expect to see, the less well a linear model will perform.

To motivate the process, I used an example of a linear model that was fit to a one-dimensional saturated proceess. This was so I could
plot the resulting "hockey stick" function, and show what happens with different model adjustments. These model adjustments
are inherently a one-dimensional procedure: find a function that adjusts the outputs of the initial model to make better predictions. So it doesn't matter what dimensionality the original problem was in. But people tend not to believe statements like that until they
see them, so here I'll apply the same procedures to a linear model with two variables, instead, just to prove my point.

## Example Problem

Here are the characteristics of our example problem:

* The feature `u` positively correlates with outcome, while `v` negatively correlates.
* The outcome `y` is constrained to be nonnegative; in other words, it "saturates" at zero.
* Both features `u` and `v` are bounded between 0 and 1 uniformly.
* There is a fairly large noise process on top of it.


```{r}
trueprocess = function(N) {
  u = runif(N)
  v = runif(N)
  noise = 0.3 * rnorm(N) 
  y = pmax(0, 4 * u - 3 * v + noise)
  data.frame(u = u, v = v, y = y)
}

# take a slice of the ideal function
# for a fixed u
ideal_uslice = function(ufixed, v) {
  pmax(0, 4*ufixed - 3*v)
}

# take a rough slice of the data
# around a fixed u
uslice = function(ufixed, processdata) {
  in_slice = abs(processdata$u - ufixed) < 0.05
  subset(processdata, in_slice)
}

rmse = function(y, ypred) {
  err = y - ypred
  sqrt(mean(err^2))
}

# we'll also calculate bias: the mean value of (y - ypred)
bias = function(y, ypred) {
  err = ypred - y
  b = mean(err)
  # let's round down to zero for small numbers
  b = ifelse(abs(b) < 1e-12, 0, b)
  b
}

```

We'll also define the procedures for model adjustment and for adjusted prediction that we showed in the previous article:

```{r}
#
# Rescale the slope of the original model's positive region
#

# outcome here is the name of the outcome column in the data frame
fit_scaler = function(initial_model, outcome, data) {
  ypred0 = predict(initial_model, newdata=data)
  df = data.frame(ypred0 = ypred0, y=data[[outcome]])
  
  # reduce the data to non-negative predictions
  df = subset(df, ypred0 > 0)
  
  # now fit the new model to the non-negative predictions
  smodel = lm(y ~ 0 + ypred0, data=df) 
  
  # return a list: initial model, adjustment model
  list(initial_model=initial_model, adjustment=smodel)
 
}

#
# Adjust the original model's positive region
# via another linear model
#

fit_linmod = function(initial_model, outcome, data) {
  ypred0 = predict(initial_model, newdata=data)
  df = data.frame(ypred0 = ypred0, y=data[[outcome]])
  
  # reduce the data to non-negative predictions
  df = subset(df, ypred0 > 0)
  
  # now fit the new model
  lpmodel = lm(y ~ ypred0, data=df) 
  
  # return a list: initial model, adjustment model
  list(initial_model=initial_model, adjustment=lpmodel)
 
}

#
# Adjust the original model's positive region
# via a spline, implemented as a GAM model
#

fit_gammod = function(initial_model, outcome, data) {
  ypred0 = predict(initial_model, newdata=data)
  df = data.frame(ypred0 = ypred0, y=data[[outcome]])
  
  # reduce the data to non-negative predictions
  df = subset(df, ypred0 > 0)
  
  # now fit the new model
  gammodel = gam(y ~ s(ypred0), data=df) 
  
  # return a list: initial model, adjustment model
  list(initial_model=initial_model, adjustment=gammodel)
 
}

#
# Make predictions using any of the above adjusted models
#

# "model" is a list (initial_model, adjustment)
do_predict = function(model, newdata) {
  mod0 = model$initial_model
  adjmod = model$adjustment
  
  df = data.frame(ypred0 = predict(mod0, newdata=newdata))
  # if the adjustment model makes negative predictions, threshold to 0
  yadj = pmax(0, predict(adjmod, newdata=df))

  # if linear model predicts a negative number,
  # predict 0, else use adjusted model
  ypred = ifelse(df$ypred0  <= 0, 0, yadj)
  ypred
}
```

## The Data

Let's generate some training data

```{r}
traind = trueprocess(1000)
head(traind)
```

We're going to use all the data to train our models, but it's not easy to show comparison plots in three dimensions in a way that's legible. So we'll also look at a slice of the data, by holding `u` fixed to 0.5 (or close to 0.5, at least).

```{r}
# the ideal process
ufixed = 0.5
v = seq(from=0, to=1, by=0.01)

# the ideal slice
idealf = data.frame(v = v, y = ideal_uslice(ufixed, v))
# a slice from the training data
slicef = uslice(ufixed, traind)

theme_set(theme_bw()) # set global ggplot theme

ggplot(mapping=aes(x=v, y=y)) + 
  geom_point(data=slicef, color="gray") +
  geom_line(data=idealf, linetype="dashed") + 
  ggtitle("Slice of training data around u=0.5", 
          subtitle="Dashed line is ideal process")
```

As you can see, the data is quite noisy, in addition to being quite saturated. 
Now let's train a linear model on the data, as well as all our adjusted models.

## Train the initial model, and all the adjustments

We'll fit the model, make the naive adjustment of thresholding the data at zero,
then fit all the candidate adjustment models. Note that only the inital model ever
makes reference to the input variables.

```{r}
# train the initial model, 
# get the initial predictions and thresholded predictions

initial_model = lm(y ~ u + v, data=traind)
traind$y_initial= predict(initial_model, newdata=traind)
traind$y_pred0 = pmax(0, traind$y_initial)

# fit the adjustment models
scaling_model = fit_scaler(initial_model, "y", traind)
linadj_model = fit_linmod(initial_model, "y", traind)
gamadj_model = fit_gammod(initial_model, "y", traind)

adjustments = list(scaling_model, linadj_model, gamadj_model)
names(adjustments) = c("y_linscale", "y_linadj", "y_gamadj")

# make the predictions
for(adj in names(adjustments)) {
  traind[[adj]] = do_predict(adjustments[[adj]], newdata=traind)
}

# pivot to a form that's better for plotting and summarization
traindlong = pivot_longer(
  traind,
  c("y_initial", "y_pred0", "y_linscale", "y_linadj", "y_gamadj"),
  names_to = "prediction_type",
  names_prefix = "y_",
  values_to = "prediction"
)

errframe = traindlong |>
  group_by(prediction_type) |>
  summarize(RMSE = rmse(y, prediction),
            bias = bias(y, prediction)) |>
  ungroup()
rownames(errframe) = errframe$prediction_type

# reorder, for presentation
morder =  c("initial", "pred0", "linscale", "linadj", "gamadj")
errframe = errframe[morder, ]
knitr::kable(errframe, caption="Model RMSE and bias on training data", row.names=FALSE)

```

As expected (if you've read the previous article), the GAM-adjusted model has the lowest training RMSE, and also lower bias than the other adjusted models. We can try to visualize what's happening, by holding `u` constant at different values and looking at slices of the prediction surfaces. 

```{r fig.width=8, fig.height=7}
uvals = c(0.1, 0.25, 0.5, 0.6, 0.75, 1)
v = seq(from=0, to=1, by=0.01)

# holding u and label fixed, return a slice
# need to add the ideal
make_slice = function(ufixed, vvec, label) {
  df = data.frame(label=label, u=ufixed, v=v)
  if (label=="initial") {
    df$ypred = predict(initial_model, newdata=df)
    return(df)
  }
  if (label=="pred0") {
    df$ypred = pmax(0, predict(initial_model, newdata=df))
    return(df)
  }
  model = adjustments[[paste0("y_",label)]]
  df$ypred = do_predict(model, newdata=df)
  df
}

# holding u fixed, return a dataframe of slices
make_uslices = function(u, vvec, labelsvec) {
  slices = lapply(labelsvec, 
                  function(label) {make_slice(u, vvec, label)})
  bind_rows(slices)
}

slices = lapply(uvals, function(u) {make_uslices(u, vvec, morder)})
slices = bind_rows(slices)

# the ideal slices
ideal = lapply(uvals, function(u) {data.frame(u=u, v=v, y=ideal_uslice(u, v))})
ideal = bind_rows(ideal)

# we can also superimpose approximate slices of the training data
datums = lapply(uvals, function(u) {
  slice = uslice(u, traind)
  slice$u = u # I need this for plotting
  slice
  })
datums = bind_rows(datums)

# todo - make the other colors a bit lighter, so gamadj pops. Maybe linadj, also
palette = c(initial = "#BBBBBB", pred0 = "#AA3377", linscale = "#228833", linadj = "#4477AA",  gamadj = "#EE6677")

ggplot() + 
  geom_point(data=datums, mapping=aes(x=v, y=y), color="gray", alpha=0.5) +
  geom_line(data=ideal, mapping=aes(x=v, y=y), linetype="dashed", color="black") + 
  geom_line(data=slices, aes(x=v, y = ypred, color=label)) + 
  facet_wrap(~u, ncol=2, scales="free_y", labeller=label_both) +
  scale_color_manual(breaks=names(palette), values=palette) + 
  ggtitle("A comparison of the adjusted model across various slices of the prediction surface",
          subtitle="Ideal process as dashed line") + 
  theme(legend.position="bottom")




```


## Test on holdout

Let's compare the models on holdout data. For this example the performances are about the same.

```{r}
testd = trueprocess(5000)

testd$y_initial= predict(initial_model, newdata=testd)
testd$y_pred0 = pmax(0, testd$y_initial)
for(adj in names(adjustments)) {
  testd[[adj]] = do_predict(adjustments[[adj]], newdata=testd)
}

# pivot to a form that's better for plotting and summarization
testdlong = pivot_longer(
  testd,
  c("y_initial", "y_pred0", "y_linscale", "y_linadj", "y_gamadj"),
  names_to = "prediction_type",
  names_prefix = "y_",
  values_to = "prediction"
)

errframe = testdlong |>
  group_by(prediction_type) |>
  summarize(RMSE = rmse(y, prediction),
            bias = bias(y, prediction)) |>
  ungroup()
rownames(errframe) = errframe$prediction_type

# reorder, for presentation
morder =  c("initial", "pred0", "linscale", "linadj", "gamadj")
errframe = errframe[morder, ]
knitr::kable(errframe, caption="Model RMSE and bias on holdout data", row.names=FALSE)
```





