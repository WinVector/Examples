{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exact Sums Over a Group Action\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In an older note ([Why No Exact Permutation Tests at Scale?](https://win-vector.com/2018/02/01/why-no-exact-permutation-tests-at-scale/)) we worked through why there can not be exact permutation tests for significance at large scale. The reason was: *if* one could efficiently perform such a test for the high degree polynomials used in exact significance calculations, then one could efficiently solve presumed hard problems such as calculating the permanent. We also recently illustrated the strength of the mean/variance characterization of the F-statistic here: [Illustrating the F-test in Action](https://github.com/WinVector/Examples/blob/main/analysis_of_variance/f_dist.ipynb). \n",
    "\n",
    "However, mean and variance are low degree [symmetric polynomials](https://en.wikipedia.org/wiki/Symmetric_polynomial). (A symmetric polynomial is a polynomial whose evaluation is not changed under re-ordering its variables.) Those should be *much* easier to deal with than general high degree polynomials.\n",
    "\n",
    "One *can* design an efficient and exact calculation of both the mean and the variance of a quality evaluation of a data partition (or clustering) under a [group action](https://en.wikipedia.org/wiki/Group_action). A \"group action\" means a calculation over permutations of the values (which is a comparative tool to show what a summary looks like when there is no relation between the partition and the data). This procedure is called a [\"permutation test\"](https://en.wikipedia.org/wiki/Permutation_test), and is a standard method in machine learning (especially with Random Forests). (Note: calculating mean and variance are not in themselves a *fully exact test*, as we would need to assume a distribution to get higher order probabilities and significances.)\n",
    "\n",
    "The goal is a dream of computer science: to identify an efficient calculation that exactly replicates the results of an expensive procedure. In this case we achieve the dream. We can exactly identify the mean and variance of such a permutation statistic, without explicitly running through all of the permutations of the data. \n",
    "Let's see this in action.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up\n",
    "\n",
    "First we import our packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import our modules/packages\n",
    "from sympy import (\n",
    "    factorial,\n",
    "    init_printing,\n",
    "    symbols,\n",
    ")\n",
    "import numpy as np\n",
    "from itertools import permutations\n",
    "from pprint import pprint\n",
    "\n",
    "from sym_calc import (\n",
    "    build_symmetric_to_moment_mapping,\n",
    "    elementary_symmetric_polynomial,\n",
    "    identify_variance_fn_regular_blocks_e,\n",
    "    theoretical_sym_poly,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disp(v):\n",
    "    display(v)\n",
    "    # print(str(v))\n",
    "\n",
    "# init_printing(pretty_print=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symmetric Polynomials\n",
    "\n",
    "A symmetric polynomial is a polynomial whose value does not change when we permute or re-arrange the variables. Some example of symmetric polynomials (in variables `y_0, y_1, y_1`) are the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elementary symmetric polynomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s1'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle y_{0} + y_{1} + y_{2}$"
      ],
      "text/plain": [
       "y_0 + y_1 + y_2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'s2'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle y_{0} y_{1} + y_{0} y_{2} + y_{1} y_{2}$"
      ],
      "text/plain": [
       "y_0*y_1 + y_0*y_2 + y_1*y_2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'s3'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle y_{0} y_{1} y_{2}$"
      ],
      "text/plain": [
       "y_0*y_1*y_2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(1, 4):\n",
    "    disp(f\"s{i}\")\n",
    "    disp(elementary_symmetric_polynomial(i, symbols(\"y_0 y_1 y_2\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The symmetric moment polynomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'m1'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle y_{0} + y_{1} + y_{2}$"
      ],
      "text/plain": [
       "y_0 + y_1 + y_2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'m2'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle y_{0}^{2} + y_{1}^{2} + y_{2}^{2}$"
      ],
      "text/plain": [
       "y_0**2 + y_1**2 + y_2**2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'m3'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle y_{0}^{3} + y_{1}^{3} + y_{2}^{3}$"
      ],
      "text/plain": [
       "y_0**3 + y_1**3 + y_2**3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(1, 4):\n",
    "    disp(f\"m{i}\")\n",
    "    disp(sum([y**i for y in symbols(\"y_0 y_1 y_2\")]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience we will write our desired results as polynomials in the symmetric moment polynomials (instead of directly in the original variables)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Example\n",
    "\n",
    "Now we set up our problem. We want to exactly run a permutation test over the quality of a partition of data in 3 blocks each of size 17."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_blocks = 3\n",
    "block_size = 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take as our example values just an increasing sequence of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = np.asarray(range(n_blocks * block_size))\n",
    "\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our proposed split into blocks is 3 blocks in contiguous order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16]),\n",
       " array([17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]),\n",
       " array([34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.split(values, n_blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point is: this partition is very informative for this data. The first block has a much lower mean than the last block. The blocks are in fact capturing as much information about the values in this data as can be done when only splitting the data into 3 groups. Typically we would have learned this split of these \"outcome\" or \"y-values\" using other explanatory variables (not shown here). And this illustration is what we would be checking in the case of a really good fit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Loss\n",
    "\n",
    "Let's define a loss function to show how bad a partition is with respect to outcome data.\n",
    "\n",
    "The function is breaking the supplied values into consecutive groups, and then calculating the square difference between items in the groups and the mean for the group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sq_loss_fn(values, *, n_blocks: int, block_size: int):\n",
    "    values = np.asarray(values)\n",
    "    assert values.shape == (n_blocks * block_size, )\n",
    "    return (\n",
    "        np.sum([(block - np.mean(block))**2 for block in np.split(values, n_blocks)]) \n",
    "        / ((block_size - 1) * n_blocks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply this loss to our data to measure how much our partition misses. We would like this value to be small, or even zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observed_loss = sq_loss_fn(values, n_blocks=n_blocks, block_size=block_size)\n",
    "\n",
    "observed_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a random arrangement (that the partition isn't good at grouping) we see a larger loss. This is evidence the original loss is small, and the partition was aware of the original data ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([42, 37, 28, 44, 26, 30, 15,  6, 29, 41,  3, 50, 47, 48,  5, 11, 14,\n",
       "        1, 25, 17, 23, 21, 46, 10, 35,  0, 33, 22, 49, 40, 18, 19,  8, 34,\n",
       "        2, 27, 16, 12, 36,  7,  4, 13, 38, 39, 32, 24, 31, 43,  9, 45, 20])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permuted_values = np.random.default_rng(25235).permutation(values)\n",
    "\n",
    "permuted_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([42, 37, 28, 44, 26, 30, 15,  6, 29, 41,  3, 50, 47, 48,  5, 11, 14]),\n",
       " array([ 1, 25, 17, 23, 21, 46, 10, 35,  0, 33, 22, 49, 40, 18, 19,  8, 34]),\n",
       " array([ 2, 27, 16, 12, 36,  7,  4, 13, 38, 39, 32, 24, 31, 43,  9, 45, 20])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.split(permuted_values, n_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225.42156862745097"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq_loss_fn(permuted_values, n_blocks=n_blocks, block_size=block_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a much larger loss. This is evidence our first arrangement was good. Unfortunately, this exact value of this large comparison loss depends on what particular permutation of the data we used. To eliminate this dependency it is traditional to average over all possible permutations. This is whole methodology is called a [permutation test](https://en.wikipedia.org/wiki/Permutation_test)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the Mean Loss\n",
    "\n",
    "Let's use a permutation test to see if this loss is small (which would be great) or large compared to a random loss.\n",
    "\n",
    "In this case, we can't try all of the permutations directly to get the exact expected loss, as there are rather a lot of permutations to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 1551118753287382280224243016469303211063259720016986112000000000000$"
      ],
      "text/plain": [
       "1551118753287382280224243016469303211063259720016986112000000000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_permutations = factorial(n_blocks * block_size)\n",
    "\n",
    "number_of_permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5511187532873822e+66"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(number_of_permutations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now compute the expected loss of permuted data over a uniform sample of the permutations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define permutation generator\n",
    "def permutation_generator(\n",
    "        values, \n",
    "        *, \n",
    "        max_samples: int = 100000,\n",
    "        rng_seed: int = 364636,\n",
    "        ):\n",
    "    \"\"\"Generate a sequence of uniformly likely permutations of values.\"\"\"\n",
    "    number_of_permutations = factorial(len(values))\n",
    "    if number_of_permutations > max_samples:\n",
    "        # sample\n",
    "        rng = np.random.default_rng(rng_seed)\n",
    "        for i in range(max_samples):\n",
    "            yield rng.permutation(values)\n",
    "    else:\n",
    "        # all\n",
    "        for perm in permutations(values):\n",
    "            yield perm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221.0366338235294"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_loss = np.mean([\n",
    "    sq_loss_fn(permuted_values, n_blocks=n_blocks, block_size=block_size) \n",
    "    for permuted_values in permutation_generator(values)\n",
    "])\n",
    "\n",
    "mean_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This says the mean-loss is probably about 221, which is much larger than our observed loss of 25.5. This is strong evidence the partition we started with *does* in fact know something about the order of the values. And we have now eliminated the dependence on choice of comparison.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The question is: can we compute the expected loss *exactly* and *without* trying to evaluate our loss function over `1.5 * 10**66` permutations?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Exact Mean Loss\n",
    "\n",
    "We now want to calculate the expected value of our loss function over all possible permutations of the data. In theory we would do this by evaluating the loss function for every permuted value of the data. This shows our original arrangement in comparison to every possible arrangement, which lets us determine scale. The magic is: we are going to calculate the exact value without performing that many steps. We will compare this to a sampled value as part of the demonstration.\n",
    "\n",
    "We can show:\n",
    "\n",
    "  * The loss polynomial summed over all permutations must be a degree 2 homogeneous [symmetric polynomial](https://en.wikipedia.org/wiki/Symmetric_polynomial). \n",
    "  * *There are not a lot of degree 2 homogeneous symmetric polynomials*. In fact they are at most a rank 2 vector space.\n",
    "  * By the above: the exact polynomial that is the sum of a given loss polynomial over all permutations can be quickly identified.\n",
    "  * [Bessel corrections](https://en.wikipedia.org/wiki/Bessel%27s_correction) (including a not shown correction for size-1 groups) ensure that the symmetrized loss polynomial depends only on the number of datums, and not the block structure!\n",
    "\n",
    "The solution in terms of symmetric moment polynomials is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - \\frac{m_{1}^{2}}{2550} + \\frac{m_{2}}{50}$"
      ],
      "text/plain": [
       "-m1**2/2550 + m2/50"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_of_loss_symmetric_polynomial = theoretical_sym_poly(n_blocks * block_size)\n",
    "symmetric_to_moments_map = build_symmetric_to_moment_mapping()\n",
    "mean_of_loss_as_moments = mean_of_loss_symmetric_polynomial.subs(symmetric_to_moments_map).simplify()\n",
    "\n",
    "mean_of_loss_as_moments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plug values into the `mean_of_loss_as_moments` polynomial to get the exact expected loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 221$"
      ],
      "text/plain": [
       "221"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = np.sum(values)\n",
    "m2 = np.sum(values**2)\n",
    "symbolic_mean_loss = mean_of_loss_as_moments.subs(\n",
    "    {'m1': m1, 'm2': m2})\n",
    "\n",
    "symbolic_mean_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm the earlier empirical `mean_loss` is in fact quite close to this idea value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_abs_error(a, b):\n",
    "    a = float(a)\n",
    "    b = float(b)\n",
    "    if a==b:\n",
    "        return 0\n",
    "    if (a==0) or (b==0):\n",
    "        return 1\n",
    "    return np.abs(a - b) / np.min([np.abs(a), np.abs(b)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert rel_abs_error(mean_loss, symbolic_mean_loss) < 1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the Variance of the Loss\n",
    "\n",
    "To take procedure a step further: let's get the variance of the loss function over the permutations. Knowing the variance will pretty much complete our characterization of the permutation test.\n",
    "\n",
    "We again start with an empirical sampling based estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.45291694978158"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_variance = np.mean([\n",
    "    (sq_loss_fn(permuted_values, n_blocks=n_blocks, block_size=block_size) - mean_loss)**2 \n",
    "    for permuted_values in permutation_generator(values)\n",
    "])\n",
    "\n",
    "loss_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Exact Variance of the Loss\n",
    "\n",
    "Now we identify the homogeneous degree 4 symmetric polynomial that will read off the exact answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{m_{1}^{4}}{3598560000} - \\frac{m_{1}^{2} m_{2}}{35280000} + \\frac{m_{1} m_{3}}{17992800} + \\frac{817 m_{2}^{2}}{1199520000} - \\frac{m_{4}}{1411200}$"
      ],
      "text/plain": [
       "m1**4/3598560000 - m1**2*m2/35280000 + m1*m3/17992800 + 817*m2**2/1199520000 - m4/1411200"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "variance_of_loss_symmetric_polynomial = identify_variance_fn_regular_blocks_e(\n",
    "    n_blocks=n_blocks, block_size=block_size)\n",
    "variance_of_loss_as_moments = variance_of_loss_symmetric_polynomial.subs(symmetric_to_moments_map).simplify()\n",
    "\n",
    "variance_of_loss_as_moments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we substitute in the evaluations of the moment polynomials to get a numeric answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{6409}{80}$"
      ],
      "text/plain": [
       "6409/80"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m3 = np.sum(values**3)\n",
    "m4 = np.sum(values**4)\n",
    "symbolic_variance = variance_of_loss_as_moments.subs(\n",
    "    {'m1': m1, 'm2': m2, 'm3': m3, 'm4': m4})\n",
    "\n",
    "symbolic_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, our previous empirical estimate is very close to the ideal answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.1125"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(symbolic_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert rel_abs_error(loss_variance, symbolic_variance) < 1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion/Discussion\n",
    "\n",
    "  * We have demonstrated exact calculation of low degree summary statistics (such as mean and variance of low degree polynomial loss function) averaged over all permutations of data. Exact mean and variance characterizations can drive powerful statistical tests. Permutations are one way to simulate a system with no useful signal (itself important for calibration).\n",
    "  * The method uses the fact that the sum of a polynomial over all permutations, must itself be a symmetric polynomial (itself immune to permutations of variables). This allows us to use elements of the theory of symmetric polynomials to complete the derivations.\n",
    "  * For a computer scientist, computing the necessary outcome of a procedure *without* actually running the original explicit defining procedure is a big deal. In this note we have computed what the average of a function over all permutations of the data would be, without explicitly running through all of the permutations.\n",
    "  * The structure of the result looks a lot like what is seen in [Polya enumeration](https://en.wikipedia.org/wiki/Pólya_enumeration_theorem) (where one writes down polynomials over moments) or discrepancy-theory/pseudo-random methods (which sometimes also break error into a structural polynomial over separately calculated moments).\n",
    "\n",
    "\n",
    "All the code for this article can be found [here](https://github.com/WinVector/Examples/tree/main/group_action).\n",
    " \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
