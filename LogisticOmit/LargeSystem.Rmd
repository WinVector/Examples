---
title: "Large System"
output: github_document
date: "2023-09-03"
always_allow_html: true
---

Work a slightly larger version of ["Solving for Hidden Data"](https://win-vector.com/2023/09/02/solving-for-hidden-data/).

Define our frame.

```{r, echo = FALSE}
# show row labels
x1_levels = c('0', '1', '2')
x2_levels = c('0', '1', '2')
y_levels = c(FALSE, TRUE)
detailed_names <- expand.grid(
  x1 = x1_levels, 
  x2 = x2_levels, 
  y = y_levels,
  stringsAsFactors = FALSE)
detailed_names <- detailed_names[
  order(detailed_names$y, 
        detailed_names$x2, 
        detailed_names$x1),
  , 
  drop = FALSE]
```

Define our explanatory variable probabilities.

```{r}
# specify explanatory variable distribution 
`P(X1=1)` <- 0.3
`P(X1=2)` <- 0.2
`P(X2=1)` <- 0.8
`P(X2=2)` <- 0.05
`P(X1=0)` <- 1 - (`P(X1=1)` + `P(X1=2)`)
`P(X2=0)` <- 1 - (`P(X2=1)` + `P(X2=2)`)
```

Define our condidtional probability of outcome given explanatory variables. 

```{r}
c0 = 0.5
b1 = 3.2
b2 = -1.1
```

Build up a data frame with row distribution defined as above.

```{r}
# assign an example outcome or dependent variable
detailed_frame <- detailed_names
detailed_frame$x1 <- as.numeric(detailed_frame$x1)
detailed_frame$x2 <- as.numeric(detailed_frame$x2)

# get joint distribution of explanatory variables
detailed_frame["P(X1=x1, X2=x2)"] <- (
  ifelse(detailed_frame$x1 == 0, `P(X1=0)`, 
         ifelse(detailed_frame$x1 == 1, `P(X1=1)`, `P(X1=2)`))
  * ifelse(detailed_frame$x2 == 0, `P(X2=0)`,
           ifelse(detailed_frame$x1 == 1, `P(X2=1)`, `P(X2=2)`)))

# converting "links" to probabilities
sigmoid <- function(x) {1 / (1 + exp(-x))}

# get conditional probability of observed outcome
# could treat x1 as categorical here
y_probability <- sigmoid(
  c0 + b1 * detailed_frame$x1 + b2 * detailed_frame$x2)

# record probability of observation
detailed_frame[["P(Y=y | X1=x1, X2=x2)"]] <- ifelse(
  detailed_frame$y, 
  y_probability, 
  1 - y_probability)

# compute joint explanatory plus outcome probability of each row
detailed_frame[["P(X1=x1, X2=x2, Y=y)"]] <- (
  detailed_frame[["P(X1=x1, X2=x2)"]] 
  * detailed_frame[["P(Y=y | X1=x1, X2=x2)"]])
```


```{r}
knitr::kable(detailed_frame)
```

Build the linear operator that maps detailed probabilities to summarized observed probabilities (where each view marginalizes out one variable).

```{r, echo = FALSE}
# build transfer matrix from joint observations to marginal observations
asterisk_symbol <- "&ast;"
margin_names <- rbind(
  expand.grid(
    x1 = x1_levels, 
    x2 = asterisk_symbol, 
    y = y_levels, 
    stringsAsFactors = FALSE),
  expand.grid(
    x1 = asterisk_symbol, 
    x2 = x2_levels, 
    y = y_levels, 
    stringsAsFactors = FALSE),
  expand.grid(
    x1 = x1_levels, 
    x2 = x2_levels, 
    y = asterisk_symbol, 
    stringsAsFactors = FALSE)
)
margin_transform <- matrix(
  data=0, 
  nrow = nrow(margin_names), 
  ncol = nrow(detailed_names))
colnames(margin_transform) <- paste0(
  "P(X1=",
  detailed_names$x1,
  ", X2=",
  detailed_names$x2,
  ", Y=",
  detailed_names$y,
  ")")
rownames(margin_transform) <- paste0(
  "P(X1=",
  margin_names$x1,
  ", X2=",
  margin_names$x2,
  ", Y=",
  margin_names$y,
  ")")
for (row_index in seq(nrow(margin_transform))) {
  for (col_index in seq(ncol(margin_transform))) {
    if (sum(detailed_names[col_index, ] == margin_names[row_index, ]) == 2) {
      margin_transform[row_index, col_index] = 1
    }
  }
}
```

```{r}
knitr::kable(margin_transform, format = "html") |>
  kableExtra::kable_styling(font_size = 8)
```

```{r}
margins <- 
   margin_transform %*% detailed_frame[["P(X1=x1, X2=x2, Y=y)"]]
```

Show a linear pre-image of the marginalization.

```{r}
(pre_image <- solve(
  qr(margin_transform, LAPACK = TRUE), 
  margins))
```

Notice this does not obey sign constraints. We will use our degrees of freedom in the pre-image process to both establish non-negativity (moving to valid likelihoods/probabilities/frequencies) and maximize entropy (under the assumption the actual solution is itself maximal entropy).

Show the degrees of freedom in the pre-image process.

```{r}
(ns <- MASS::Null(t(margin_transform)))
```

The above null vectors include the [Kronecker product](https://en.wikipedia.org/wiki/Kronecker_product) of the per-variable null spaces. Each of the per-variable null spaces are just a basis for the orthogonal complement of an all ones vector of length equal to the number of values the variable takes. So *if* we establish that these are *all* of the null-vectors, we expect the null space to be of dimension the product of each variables number of levels each minus 1. That would be why in ["Solving for Hidden Data"](https://win-vector.com/2023/09/02/solving-for-hidden-data/) the null space is rank 1, and here it is rank 4.

Define our "pick a good solution" objective function.

```{r}
entropy <- function(v) {
  v <- v[v > 0]
  if (length(v) < 2) {
    return(0)
  }
  v <- v / sum(v)
  -sum(v * log2(v))
}
```

Solve: maximize `entropy(pre_image + ns * z)` subject to `pre_image + ns * z >= 0`.

```{r}
library(lpSolve)
```

Get an interior starting solution for the later optimizer.

```{r}
# solve for possible interior solution by maximizing slack
# this call takes only non-negative variables, so we double
# the variables to simulate signed variables.
soln0 <- lp(
  direction = "max",
  objective.in = c(rep(0, ncol(ns) * 2), 1),
  const.mat = cbind(ns, -ns, -1),
  const.dir = rep(">=", nrow(ns)),
  const.rhs = -pre_image,
)
z0 <- soln0$solution[1:ncol(ns)] - soln0$solution[(1+ncol(ns)):(2*ncol(ns))]

z0
```

```{r}
recovered0 <- ns %*% z0 + pre_image
stopifnot(
  all(recovered0 > 0)
)
stopifnot(
  max(abs(margins - margin_transform %*% recovered0)) < 1e-6
)

recovered0
```

```{r}
entropy(recovered0)
```

Solve the constrained optimization problem.

```{r}
# maximize entropy in constrained region
f <- function(z) {-entropy(ns %*% z + pre_image)}
g <- function(z) {
  eps = 1e-8
  n = length(z)
  f_val = f(z)
  ret = rep(0, n)
  for (i in seq(n)) {
    zi = z
    zi[i] = z[i] + eps
    ret[i] = (f(zi) - f_val)/eps
  }
  return(ret)
}
soln1 <- constrOptim(
  theta = z0,
  f = f,
  grad = g,
  ui = ns,
  ci = -pre_image,
  outer.iterations = 1000,
  outer.eps = 1e-10,
  method = "CG"
)
z1 <- soln1$par

z1
```



```{r}
recovered1 <- ns %*% z1 + pre_image
stopifnot(
  all(recovered1 >= 0)
)
stopifnot(
  max(abs(margins - margin_transform %*% recovered1)) < 1e-6
)

recovered1
```

```{r}
entropy(recovered1)
```

```{r}
stopifnot(
  entropy(recovered1) >= entropy(recovered0)
)
```

This recovered entropy should be at least as high as the unobserved actual solution (modulo optimization early stopping and numeric issues). If they are equal they are the same solution.

```{r}
entropy(detailed_frame[["P(X1=x1, X2=x2, Y=y)"]])
```

```{r}
stopifnot(
  entropy(recovered1) - entropy(detailed_frame[["P(X1=x1, X2=x2, Y=y)"]]) > -1e-6
)
```

We show this solution is very close to the original (unobserved) distribution. I.e.: we have solved the problem.

```{r}
(residuals <- 
  detailed_frame[["P(X1=x1, X2=x2, Y=y)"]] - recovered1)
```

```{r}
stopifnot(
  max(abs(residuals)) < 1e-5
)
```

What we want to show is that the distribution given is highest entropy with respect to the variations of the specified adjustment vectors. If this is the case, then we *always* recover the actual pre-image distribution through these methods.


