---
title: "margin_transform"
output: github_document
date: "2023-08-24"
---

## Introduction

Let's continue along the lines discussed in [Omitted Variable Effects in Logistic Regression](https://win-vector.com/2023/08/18/omitted-variable-effects-in-logistic-regression/).

## Our Example

For our example we set up a logistic regression on two explanatory variables `x1` and `x2` . For simplicity we will take the case where `x1` and `x2` take on the values `0` and `1`.

Our data is then keyed by the values of these explanatory variables and the dependent or outcome variable `y`. The keying looks like the following.

```{r}
# show row labels
(detailed_names <- expand.grid(
  x1 = c('0', '1'), 
  x2 = c('0', '1'), 
  y = c(FALSE, TRUE), 
  stringsAsFactors = FALSE))
```

Let's specify the joint probabilty distribution of our two explanatory variables. We choose them as independent with the following expected values.

```{r}
# specify explanatory variable distribution 
pX1 = 0.3
pX2 = 0.8
```

Our data set can then be completely described by above explanatory variable distribution *and* the conditional probability of the dependent outcomes. For our logistic regression problem we set up our outcome conditioning coefficients as follows.

```{r}
# 0.5772
(cval <- -digamma(1))
```

```{r}
# 3.1415
(b1 <- pi)
```

```{r}
# 27.182
(b2 <- - 3 * exp(1))
```

Please remember these coefficients in this order for later.

```{r}
# show constants in an order will see again
c(cval, b1, b2)
```

Using the methodology of [Replicating a Linear Model](https://win-vector.com/2019/07/03/replicating-a-linear-model/) we can build a data set that obeys the specified explanatory variable distribution and has specified outcome probabilities. This is just us building a data set matching an assumed known answer.

```{r}
# assign an example outcome or dependent variable
detailed_frame <- detailed_names
detailed_frame$x1 <- as.numeric(detailed_frame$x1)
detailed_frame$x2 <- as.numeric(detailed_frame$x2)
# get joint distribution of explanatory variables
detailed_frame$x_distribution <- (
  (detailed_frame$x1 * pX1 + (1 - detailed_frame$x1) * (1 - pX1))
    * (detailed_frame$x2 * pX2 + (1 - detailed_frame$x2) * (1 - pX2))
)
# get conditional probability of observed outcome
y_linear <- cval + b1 * detailed_frame$x1 + b2 * detailed_frame$x2
# converting "links" to probabilities
sigmoid <- function(x) {1 / (1 + exp(-x))}
y_probability <- sigmoid(y_linear)
detailed_frame$p_observed_outcome <- ifelse(detailed_frame$y, y_probability, 1 - y_probability)
# compute joint explanatory plus outcome probability of each row
detailed_frame$proportion <- detailed_frame$x_distribution * detailed_frame$p_observed_outcome
stopifnot(abs(sum(detailed_frame$proportion) - 1) < 1e-6)
```



```{r}
knitr::kable(detailed_frame)
```

```{r}
# clear some columns we are no longer using
detailed_frame$x_distribution <- NULL
detailed_frame$p_observed_outcome <- NULL
```


We can confirm this data set encodes the expected logistic relationship.

```{r}
# suppressWarnings() to avoid fractional data weight complaint
correct_coef <- suppressWarnings(
  glm(
    y ~ x1 + x2,
    data = detailed_frame,
    weights = detailed_frame$proportion,
    family = binomial()
  )$coef
)

correct_coef
```

Notice we recover the `cval + b1 * detailed_frame$x1 + b2 * detailed_frame$x2` form.

## The Problem

Now let's get introduce our issue. Suppose we have two experimenters, each of which only observing one of the explanatory variables? As we saw in [Omitted Variable Effects in Logistic Regression](https://win-vector.com/2023/08/18/omitted-variable-effects-in-logistic-regression/) each of these experimenters will in fact estimate coefficients that are biased towards zero, due to the non-collapsibility of the modeling set up. This differs from linear regression where for independent explanatory variables (as we have here) we would expect each experimenter to be able to get an unbiased estimate of the explanatory variable available to them!

To show this let's build a linear operator that computes the marigns the experimenters actually observe.


```{r}
# build transfer matrix from joint observations to marginal observations
margin_names <- rbind(
  expand.grid(x1 = c('0', '1'), x2 = '*', y = c(FALSE, TRUE), stringsAsFactors = FALSE),
  expand.grid(x1 = '*', x2 = c('0', '1'), y = c(FALSE, TRUE), stringsAsFactors = FALSE),
  expand.grid(x1 = c('0', '1'), x2 = c('0', '1'), y = '*', stringsAsFactors = FALSE)
)
margin_transform <- matrix(data=0, nrow = nrow(margin_names), ncol = nrow(detailed_names))
colnames(margin_transform) <- paste0("n(", apply(detailed_names, 1, paste, collapse = ","), ")")
rownames(margin_transform) <- paste0("n(", apply(margin_names, 1, paste, collapse = ","), ")")
for (row_index in seq(nrow(margin_transform))) {
  for (col_index in seq(ncol(margin_transform))) {
    if(sum(detailed_names[col_index, ] == margin_names[row_index, ]) == 2) {
      margin_transform[row_index, col_index] = 1
    }
  }
}

knitr::kable(margin_transform)
```


```{r}
# apply the linear operator to compute marginalized observations
margins <- as.numeric(margin_transform %*% detailed_frame$proportion)
margin_frame <- margin_names
margin_frame$proportion <- margins

knitr::kable(margin_frame)
```

### Experimenter 1's view

Experimenter 1 only knows about the rows named `ln(0/1, *, FALSE/TRUE)`. Experimenter 2 only knows about the rows named `ln(*, 0/1, FALSE/TRUE)`. Neither experimenter knows the `ln(0/1, 0/1, *)` rows, but if they pool their data they can in fact use the independence assumption to recover these rows.

Let's see what happens when each experimenter tries to perform inference on their fraction of the data.

```{r}
# select data available to d1
d1 <- margin_frame[margin_frame$x2 == '*', , drop = FALSE]
d1$x1 <- as.numeric(d1$x1)
d1$y <- as.logical(d1$y)
```

```{r}
# solve from d1's point of view
suppressWarnings(
  glm(
    y ~ x1,
    data = d1,
    weights = d1$proportion,
    family = binomial()
  )$coef
)
```

Notice experimenter 1 got a way too small extimate of the `x1` coefficient.

### Experimenter 2's view

```{r}
# select data available to d2
d2 <- margin_frame[margin_frame$x1 == '*', , drop = FALSE]
d2$x2 <- as.numeric(d2$x2)
d2$y <- as.logical(d2$y)
```


```{r}
# solve from d2's point of view
suppressWarnings(
  glm(
    y ~ x2,
    data = d2,
    weights = d2$proportion,
    family = binomial()
  )$coef
)
```

Experimenter 2 also gets a wrong estimate, but closer as the the omitted `x1` variable had less influence.

## The Question

From the original data set's point of view: both experimenters have wrong estimates of their respective coefficients. The question then is: if the experimenters pool their effort can they infer the correct coefficients?

Each experimenter knows a lot about the data. They known the distribution of their explanatory variable, and even the joint distribution of their explanatory and the dependent and outcome data. Assuming the two explanatory variables are independent, they even know the joint distribution of the explanatory variables. Together they *almost* know the original non-marginalized data distribution. Let's try to solve for an estimate of that distribution given the data available when the experimenters pool their observations.

## A Solution

What we are going to do is: simulate experimenter 1 and experimenter 2 working together to try and recover a joint estimate of the `x1` and `x2` coefficients. We are going to use their pooled information to guess at plausible pre-images that represent the unobserved joint data set that has simultaneous `x1` and `x2` observations. This is kind of cute: instead of trying to invert the estimate bias, we try and guess at original data wher we know how to perform an unbiased analysis.

We characterize all pre-images of the pooled marginal information. These are all of the form of a pre-image of the observed `margins` vector plus any elements of the marginalization processes' null-space (i.e. changes in data that are not respected by the summation process).

In our case we have an example data distribution (with illegal negative entries!).

```{r}
# typical solution (in the linear sense, signs not enforced)
v <- as.numeric(MASS::ginv(margin_transform) %*% margins)

v
```

And we have a single element of the null space. 

```{r}
# our degree of freedom between solutions
ns <- MASS::Null(t(margin_transform))
stopifnot(ncol(ns) == 1)
ns <- as.numeric(ns)

ns
```

So all valid solutions are of the form `v + z * ns` for scalars `z`. In fact all solutions are some interval of `z` values. Let's solve for them.

```{r}
# solve for the z's
proposed_solns <- unique(as.numeric(- v / ns )[abs(ns) > 1e-8])
solns <- proposed_solns[vapply(proposed_solns, function(soln) {all(soln * ns + v >= -1e-8)}, logical(1))]
solns <- unique(c(min(solns), max(solns)))
stopifnot(length(solns) > 0)
```

```{r}
# solve the logistic regression for each of our extreme guessed pre-images of the data
soln_i <- 0
soln_names <- rep('', length(solns))
for (soln in solns) {
  recovered <- as.numeric(soln * ns + v)
  recovered <- pmax(recovered, 0)  # get rid of any very near zero entries
  soln_i <- soln_i + 1
  soln_name <- paste0("recovered_", soln_i)
  detailed_frame[soln_name] <- recovered
  soln_names[soln_i] <- soln_name
}
```

Our attempted recovered solutions to the (unknown to either experimenter!) original data distribution details can be seen below.

```{r}
knitr::kable(detailed_frame)
```

As we can see these two extreme solutions are in fact actually fairly close. And the original (unobserved) data distribution is in fact a convex combination of these solutions.

```{r}
# confirm actual proportions in convex hull of solutions
convex_soln <- lm(
  p ~ 0 + r1 + r2, 
  data = data.frame(
    r1 = detailed_frame$recovered_1, 
    r2 = detailed_frame$recovered_2, 
    p = detailed_frame$proportion)
)
stopifnot(max(abs(convex_soln$residuals)) < 1e-8)
stopifnot(all(convex_soln$coefficients > -1e-8))
stopifnot(abs(sum(convex_soln$coefficients) - 1) < 1e-8)
```

And, in this case, it turns out each of our extreme solutions recovers essentially the same logistic regression solution! And this matches the unobserved detailed solution! Likely the one degree of freedom in the solution space matches some of the logistic regression balance conditions, and is therefore a unimportant or indifferent source of variation.

```{r}
# remind ourselves of the correct solution from actual (unobserved) joint data
correct_coef
```

```{r}
# run through our recovered solutions
soln_i <- 0
coef_list <- as.list(rep(NULL, length(soln_names)))
for (soln_name in soln_names) {
  print(soln_name)
  soln <- as.numeric(detailed_frame[[soln_name]])
  m_rec <- as.numeric(margin_transform %*% soln)
  max_error <- max(abs(m_rec - margins))
  stopifnot(max_error < 1e-8)
  coef <- suppressWarnings(
    glm(
      y ~ x1 + x2,
      data = detailed_frame,
      weights = detailed_frame[[soln_name]],
      family = binomial()
    )$coef
  )
  print("recovered solution")
  print(coef)
  soln_i <- soln_i + 1
  coef_list[[soln_i]] <- coef
}
```

### Picking a point-estimate

It is convenient to pick a distinguished or "best guess" solution. In our case here it doesn't matter much, as both our extreme solutions can nearly identical logistic regression inferences. However if we do want to pick a single guess at the data pre-image the usual criterion is pick the maximum entropy one. 

This is just a simple principle: prefer flat distributions until one have evidence against them. This modeling techique is itself very strongly related to logistic regression modeling.

```{r}
# brute force solve for maximum entropy mix
# obviously this can be done a bit slicker
entropy <- function(v) {
  v <- v[v > 0]
  v <- v / sum(v)
  -sum(v * log2(v))
}

opt_soln <- optimize(
  function(z) {entropy(z * detailed_frame$recovered_1 + (1 - z) * detailed_frame$recovered_2)},
  c(0, 1),
  maximum = TRUE)

z_opt <- opt_soln$maximum
stopifnot(z_opt >= 0)
stopifnot(z_opt <= 1)
detailed_frame["opt_dist"] <- z_opt * detailed_frame$recovered_1 + (1 - z_opt) * detailed_frame$recovered_2
```

Notice that the recoved `opt_dist` is pretanaturally close to the unobserved original `proportion`.

```{r}
knitr::kable(detailed_frame)
```

And we feel confident with the following estimated coefficients.

```{r}
coef <- suppressWarnings(
  glm(
    y ~ x1 + x2,
    data = detailed_frame,
    weights = detailed_frame[["opt_dist"]],
    family = binomial()
  )$coef
)
print("recovered solution")
print(coef)
```



## Conclusion

The point is: by pooling their observations the two researchers can recover a very good estimate of the joint analysis neither of them performed. The strategy we used is: try to estimate plausible pre-images of the data they saw and analyze that. This in fact gives us how to invert the bias introduced by the omitted variable observations.

In the real world the two experimenters at best would be looking at marginalizations of different draws of related data. So we would not have exact matches we can invert- but instead would have to guess low-discrepancy pre-images of the data. And, as always, as we are now introducing a lot of unobserved data we could got to Bayesian graphical model methods to sum this out (instead of proposing a specific pointwise heuristic as we did here).

