{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell0",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Stan solution to [nested_model_example.ipynb](nested_model_example.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up Python\n",
    "import re\n",
    "import json\n",
    "import inspect\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown\n",
    "import plotnine\n",
    "from plotnine import *\n",
    "\n",
    "from nested_model_fns import (\n",
    "    generate_Stan_model_def,\n",
    "    solve_forecast_by_Stan,\n",
    "    plot_forecast,\n",
    "    extract_sframe_result,\n",
    "    plot_decomposition,\n",
    "    plot_model_quality,\n",
    "    plot_model_quality_by_prefix,\n",
    "    plot_params,\n",
    "    plot_past_and_future,\n",
    ")\n",
    "\n",
    "# quiet down Stan\n",
    "logger = logging.getLogger(\"cmdstanpy\")\n",
    "logger.addHandler(logging.NullHandler())\n",
    "\n",
    "# set plot size\n",
    "plotnine.options.figure_size = (16, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e789baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"generating_params.json\", \"r\") as file:\n",
    "    generating_params = json.load(file)\n",
    "modeling_lags = generating_params[\"generating_lags\"]\n",
    "b_z = generating_params[\"b_z\"]\n",
    "b_x = generating_params[\"b_x\"]\n",
    "\n",
    "generating_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172404a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = pd.read_csv(\"d_train.csv\")\n",
    "d_test = pd.read_csv(\"d_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell2",
   "metadata": {},
   "source": [
    "## Solving again with the Bayesian \"big hammer\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell3",
   "metadata": {},
   "source": [
    "We now try a Bayesian model with the correct generative structure, using the [Stan](https://mc-stan.org/users/interfaces/cmdstan) software package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a Stan model for both transient external regressors and future predictions\n",
    "stan_model_with_forecast_src_i = generate_Stan_model_def(\n",
    "        application_lags=modeling_lags,\n",
    "        n_transient_external_regressors=len(b_x),\n",
    "        n_durable_external_regressors=len(b_z),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the model specification\n",
    "display(Markdown(f\"```stan\\n{stan_model_with_forecast_src_i}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell6",
   "metadata": {},
   "source": [
    "`y` and `y_auto` are supposed to be non-negative (a constraint we have chosen to *not* enforce, as it degraded results, probably by damaging sampling paths).\n",
    "\n",
    "Please keep in mind a distributional statement such as <code>y ~ normal(f(y_auto, x), &sigma;)</code> is actually modeling the residual <code>(y - f(y_auto, x))</code> as being distributed <code>normal(0, &sigma;)</code>. So the above model-block statements are distributional assumptions about *residuals*, as the intended mean is an input to these statements. Thus we are specifying a normal distribution for residuals, not a normal distribution for expected values or predictions. I feel the normal approximation for `y`'s residual is not that bad. A similar statement can be made for `y_auto`'s residuals.\n",
    "\n",
    "All in all: specifying systems to Stan is a compromise in respecting problem structure, and preserving the ability to effectively sample. The specification tends to requires some compromise and experimentation. In my opinion, it isn't quite the case that \"Bayes' Law names only one legitimate inferential network and we can then use that one!\" One is going to have to specify an approximate system. It becomes the user's responsibility to design for a (hopefully) high utility tradeoff between fidelity and realizability.\n",
    "\n",
    "For model simplicity and legibility we have not treated observed `y = 0` as censored `y <= 0` observations as in [\"Post-hoc Adjustment for Zero-Thresholded Linear Models\"](https://win-vector.com/2024/08/16/post-hoc-adjustment-for-zero-thresholded-linear-models/). This could easily be incorporated into a production model.\n",
    "\n",
    "We obviously will not know all the priors. So we hope the problem is somewhat insensitive to them and just set them to not so bad distributions. It is possible to over-worry on priors, and somewhat freeing to just think of them as [regularizations](https://en.wikipedia.org/wiki/Regularization_(mathematics)) or biases for the values in question to be small. Also it sometimes makes sense to fight symmetries or degeneracies in the specification by adding \"complementarity\" constraints such as `b_auto_0 * b_imp_0 ~ normal(0, 0.1)`. This is not a distributional claim we believe, but a trick in saying we expect the product to be small to enforce we expect only one of the two values to be non-negligible. Again, think of the model distributional claims as \"criticisms\" and not if they are prior (before seeing data) or posterior (after seeing data) opinions. Also, don't be profligate with these exotic checks: they break convexity of the function we are optimizing and can make sampling harder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell7",
   "metadata": {},
   "source": [
    "What Stan generates is: thousands of possible trajectories of parameters, and past and future hidden state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from Stan model solutions\n",
    "forecast_soln_i = solve_forecast_by_Stan(\n",
    "    model_src=stan_model_with_forecast_src_i,\n",
    "    d_train=d_train,\n",
    "    d_apply=d_test,\n",
    "    durable_external_regressors=[\"z_0\"],\n",
    "    transient_external_regressors=[\"x_0\"],\n",
    "    cache_file_name='forecast_soln_i.csv.gz',\n",
    ")\n",
    "\n",
    "forecast_soln_i  # see https://mc-stan.org/docs/cmdstan-guide/stansummary.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8692766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp__q_90 = forecast_soln_i['lp__'].quantile(q=0.9)\n",
    "lp__sd = forecast_soln_i['lp__'].std()\n",
    "log_samples = np.log(forecast_soln_i.shape[0])\n",
    "(\n",
    "    ggplot(\n",
    "        data=forecast_soln_i,\n",
    "        mapping=aes(x='lp__')\n",
    "    )\n",
    "    + geom_density(fill=\"darkgrey\", alpha=0.5)\n",
    "    + geom_vline(xintercept=lp__q_90, linetype=\"dashed\")\n",
    "    + ggtitle(f\"distribution of pseudo log-likelihood\\nstd-dev: {lp__sd:0.2g}, log samples: {log_samples:0.2g}\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be35ede",
   "metadata": {},
   "source": [
    "`lp__` is a shifted copy of the log-likelihood. We expect in most situations the distribution of `lp__` will be unimodal and to have a standard deviation not larger than `k log(n_samples)` for some small integer `k`. The first point is fairly technical and can be related to how fast volumes grow in large dimension (similar ideas include [the asymptotic equipartition theorem](https://en.wikipedia.org/wiki/Asymptotic_equipartition_property), [the law of the iterated logarithm](https://en.wikipedia.org/wiki/Law_of_the_iterated_logarithm)). The second part is simpler: with only `n_samples` we don't expect to often see events much rarer than `1/n_samples`, so if we trajectories with log-likelihood `lp__ + c` then we shouldn't see a lot of trajectories with log-likelihood `lp__ + c - k * log(n_samples)`.  So when we do see multimodality and/or wide dispersions of `lp__`, we do have a pathology. This is one of our motivations in filtering to more likely samples, in addition to these samples representing better fits.\n",
    "\n",
    "Note: one can over-play this. Stan samples are designed so that <code>(sum<sub>i sample</sub> v<sub>i</sub>) / (sum<sub>i sample</sub> 1)</code> approximates <code>E[v]</code>, not <code>(sum<sub>i sample</sub> exp(lp__) v<sub>i</sub>) / (sum<sub>i sample</sub> exp(lp__))</code>. I.e. the samples are already *implicitly* weighted by how often the sampling system finds them, so plugging in the likelihood again is not fully justified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07c9ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a trick I like: limit down to the more plausible samples/trajectories\n",
    "forecast_soln_i = (\n",
    "    forecast_soln_i.loc[\n",
    "        forecast_soln_i['lp__'] >= lp__q_90,\n",
    "        :\n",
    "    ].reset_index(drop=True, inplace=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7257a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lp__sd = forecast_soln_i['lp__'].std()\n",
    "log_samples = np.log(forecast_soln_i.shape[0])\n",
    "(\n",
    "    ggplot(\n",
    "        data=forecast_soln_i,\n",
    "        mapping=aes(x='lp__')\n",
    "    )\n",
    "    + geom_density(fill=\"darkgrey\", alpha=0.5)\n",
    "    + geom_vline(xintercept=lp__q_90, linetype=\"dashed\")\n",
    "    + ggtitle(f\"distribution of filtered pseudo log-likelihood\\nstd-dev: {lp__sd:0.2g}, log samples: {log_samples:0.2g}\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell9",
   "metadata": {},
   "source": [
    "We can look at a summary of the parameter estimates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize parameter estimates\n",
    "soln_params_i = forecast_soln_i.loc[\n",
    "    :, [c for c in forecast_soln_i if c.startswith(\"b_\")]\n",
    "].median()\n",
    "\n",
    "soln_params_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show original generative parameters\n",
    "generating_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c257767b",
   "metadata": {},
   "source": [
    "Notice we recover `b_x_dur ~ b_z` and `b_x_imp ~ b_x` pretty well. These effect inferences can be used for planning and policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_params(\n",
    "    forecast_soln_i=forecast_soln_i,\n",
    "    generating_params=generating_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell13",
   "metadata": {},
   "source": [
    "And we can plot both the forecasts, and *estimated* quantile bands around the estimated forecasts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot inference over time\n",
    "plt_i, s_frame_i = plot_forecast(\n",
    "    forecast_soln_i,\n",
    "    d_test,\n",
    "    model_name=\"Stan correct externals model\",\n",
    "    external_regressors=[\"z_0\", \"x_0\"],\n",
    ")\n",
    "plt_i.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7851cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_soln_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c046411",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1, p2 = plot_past_and_future(forecast_soln_i=forecast_soln_i, d_train=d_train, d_test=d_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f500f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808ac0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot quality of fit as a scatter plot\n",
    "d_test[\"Stan (correct externals structure) prediction\"] = extract_sframe_result(\n",
    "    s_frame_i\n",
    ")\n",
    "plot_model_quality(\n",
    "    d_test=d_test,\n",
    "    result_name=\"Stan (correct externals structure) prediction\",\n",
    "    external_regressors=[\"z_0\", \"x_0\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot quality as a function of how far out we are predicting\n",
    "plot_model_quality_by_prefix(\n",
    "    s_frame=s_frame_i,\n",
    "    d_test=d_test,\n",
    "    result_name=\"Stan (correct externals structure) prediction\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell17",
   "metadata": {},
   "source": [
    "The model identifies 3 very valuable things:\n",
    "\n",
    "  * Estimates of the model parameters: \n",
    "     * `b_auto_0`\n",
    "     * `b_auto[0]`\n",
    "     * `b_auto[1]`\n",
    "     * `b_x_dur[0]`\n",
    "     * `b_x_imp[0]`\n",
    "  * Projections or applications of the model for future `time_tick`s 950 through 999.\n",
    "  * Good inferences of the most recent unobserved states `y_auto[948]` and `y_auto[949]` in the training period.\n",
    "\n",
    "The third item provides a much more useful estimate of then hidden state (based on evaluation of trajectories through the entire training period) than the simple single point estimate `y_auto[i] ~ y[i] * b_x_impl[0] - x_0[i]`. One can evolve estimates forward from these inferences, and that is not always the case for the simple expected value estimates.\n",
    "\n",
    "The issue with the simple (or naive) estimates being: they are single value point estimates, not necessarily compatible with *any* of the estimate sampling trajectories. Plugging in the naive estimates often does not allow one to evolve the prediction trajectories forward in a sensible manner. The detailed estimates from the Stan sampler do allow such forward evolution of estimates (either inside the Stan sampler as shown, or as a simple external procedure).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23d5b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decomposition(\n",
    "    forecast_soln_i=forecast_soln_i,\n",
    "    d_train=d_train,\n",
    "    d_test=d_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96faa661",
   "metadata": {},
   "source": [
    "The calling sequence to do this in Stan from Python is as follows. One can also call Stan from the command line or from R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30065de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"```python\\n{inspect.getsource(solve_forecast_by_Stan)}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a843a6",
   "metadata": {},
   "source": [
    "Even external regressors that we don't know future values of can be useful. We can use them to de-bias model parameter estimation, and then without the future values the model is a golden path \"if nothing happens\" scenario. Projecting the projection forward with a few guessed external values can give sensitivity analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f0b59c",
   "metadata": {},
   "source": [
    "The downsides of Stan include\n",
    "\n",
    "  * It is slow and expensive to run.\n",
    "  * It is non-deterministic, different runs may achieve different quality of results (requiring still more runs).\n",
    "  * We saw failing runs in preparing this example (indicating need for longer run times to convergence)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell31",
   "metadata": {},
   "source": [
    "## Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell32",
   "metadata": {},
   "source": [
    "And that concludes our note on modeling in the presence of external regressors. The main point is: one has to specify the structure of the regressors. Do they cause durable effects (such as marketing efforts) or do they cause transient effects (such as one-off sales events)? Also: we would like such specifications to be in terms familiar to domain experts, and not deep in ARMAX or transfer function terminology.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prob_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
